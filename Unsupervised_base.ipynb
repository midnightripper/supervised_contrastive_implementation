{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm6CrX-XDs02",
        "outputId": "7bf1b09f-e6b8-4ece-fffa-dc037582af36"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "from keras import backend as K\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "79UUK0g7D8BA"
      },
      "outputs": [],
      "source": [
        "def dataloader(path, featType):\n",
        "    data = scipy.io.loadmat(path)\n",
        "    print(data.keys())\n",
        "\n",
        "    AF = data['AF']\n",
        "    x1 = AF[:-2]\n",
        "    y = AF[-2]\n",
        "    w = AF[-1]\n",
        "\n",
        "    if featType == 1:\n",
        "        x = x1\n",
        "    else:\n",
        "        x2 = data['CF']\n",
        "        x = np.concatenate((x1, x2), axis=0)\n",
        "    return x.T, y.T, w.T, data['CF_info']\n",
        "\n",
        "def calculate_accuracy(arr1, arr2):\n",
        "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
        "    return count / len(arr1)\n",
        "\n",
        "def normalization(feats):\n",
        "  df = pd.DataFrame(feats)\n",
        "  scaler = StandardScaler()\n",
        "  x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "  return x_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1qgHWXIE7XE"
      },
      "source": [
        "#Flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5XhgKpZ_EwZR"
      },
      "outputs": [],
      "source": [
        "def make_partitions(arr_words, arr_labels):\n",
        "    v = []\n",
        "    temp = []\n",
        "\n",
        "    for i in range(len(arr_words) - 1):\n",
        "        word = arr_words[i]\n",
        "        next_word = arr_words[i + 1]\n",
        "        temp.append(arr_labels[i])\n",
        "\n",
        "        if word != next_word or i == len(arr_words) - 2:\n",
        "            if i == len(arr_words) - 2:\n",
        "                temp.append(arr_labels[i + 1])\n",
        "\n",
        "            numpy_temp = np.array(temp)\n",
        "            temp_max = np.amax(numpy_temp)\n",
        "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
        "            v = np.concatenate((v, numpy_temp), axis=None)\n",
        "            temp.clear()\n",
        "\n",
        "    v1 = [1 if i == 1 else 0 for i in v]\n",
        "    return v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "fatyp = 'TypicalFA_comb1'\n",
        "drivepath = 'finalData/'+ fatyp +'/';\n",
        "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
        "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
        "featType = 1; #Acoustic or Acoustic+context\n",
        "if featType == 1:\n",
        "  original_dim = 19\n",
        "else:\n",
        "  original_dim = 38"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
          ]
        }
      ],
      "source": [
        "train_path = filee; test_path = filee.replace('train','test')\n",
        "# print('test file:::::::',os.path.basename(test_path))\n",
        "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
        "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
        "\n",
        "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
        "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "xtest_a = normalization(xtest)\n",
        "xtest_ac = normalization(xtest1)\n",
        "\n",
        "xtrain = normalization(xtrain)\n",
        "xtrain1 = normalization(xtrain1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5891, 19)\n"
          ]
        }
      ],
      "source": [
        "print(xtrain.shape)\n",
        "woPP=[]; wPP=[]\n",
        "\n",
        "input_shape1 = (19,)\n",
        "input_shape2 = (38,)\n",
        "\n",
        "temperature = 0.03\n",
        "learning_rate=0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=temperature, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        print(feature_vectors.shape)\n",
        "        # labels = tf.keras.layers.Concatenate(axis=0)([labels, labels])\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = tf.divide(tf.matmul(feature_vectors_normalized, tf.transpose(feature_vectors_normalized)), self.temperature)\n",
        "        \n",
        "        # print(feature_vectors.shape)\n",
        "        # print(labels.shape)\n",
        "        # print('loss:::::::', tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
        "        return 0.35*(tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
        "        #find out more about why 0.35 is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Input\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.layers import Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_encoder1(latent_dim):\n",
        "    encoder_input = Input(shape=(19,1))\n",
        "    x1 = Conv1D(filters=64, kernel_size=2, activation='relu')(encoder_input)\n",
        "    x2 = MaxPooling1D(pool_size=1)(x1)\n",
        "    x3 = Dropout(0.5)(x2)\n",
        "    x4 = Conv1D(filters=32, kernel_size=2, activation='relu')(x3)\n",
        "    x5 = MaxPooling1D(pool_size=1)(x4)\n",
        "    x6 = Flatten()(x5)\n",
        "    output_layer = Dense(latent_dim, activation='gelu')(x6)\n",
        "    encoder1 = Model(encoder_input, output_layer)\n",
        "    return encoder1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_encoder2(latent_dim):\n",
        "    encoder_input = Input(shape=(38,1))\n",
        "    x1 = Conv1D(filters=64, kernel_size=2, activation='relu')(encoder_input)\n",
        "    x2 = MaxPooling1D(pool_size=1)(x1)\n",
        "    x3 = Dropout(0.5)(x2)\n",
        "    x4 = Conv1D(filters=32, kernel_size=2, activation='relu')(x3)\n",
        "    x5 = MaxPooling1D(pool_size=1)(x4)\n",
        "    x6 = Flatten()(x5)\n",
        "    output_layer = Dense(latent_dim, activation='gelu')(x6)\n",
        "    encoder2 = Model(encoder_input, output_layer)\n",
        "    return encoder2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_projection_head1(Encoder1, Encoder2):\n",
        "    inp1 = keras.Input(shape=input_shape1)\n",
        "    inp2 = keras.Input(shape=input_shape2)\n",
        "    hidden3a  = Encoder1(inp1)\n",
        "    hidden3b = Encoder2(inp2)\n",
        "    features = tf.keras.layers.Concatenate(axis=1)([hidden3a, hidden3b])\n",
        "    features = layers.Dense(16, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=[inp1, inp2], outputs=features, name=\"cifar-encoder_with_projection-head\"\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_classifier(encoder, trainable):\n",
        "\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    inputs1 = keras.Input(shape=input_shape1)\n",
        "    inputs2 = keras.Input(shape=input_shape2)\n",
        "    features1 = encoder1(inputs1)\n",
        "    features2 = encoder2(inputs2)\n",
        "    features = tf.keras.layers.Concatenate(axis=1)([features1, features2])\n",
        "    # features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
        "    \n",
        "    features = layers.BatchNormalization()(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(32, activation=\"relu\")(features)\n",
        "    features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(16, activation=\"relu\")(features)\n",
        "    # features = layers.BatchNormalization()(features)\n",
        "    # features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(4, activation=\"relu\")(features)\n",
        "    # features = layers.BatchNormalization()(features)\n",
        "    # features = layers.Dropout(0.1)(features)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = xtrain.shape[0]\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1178, 19)\n",
            "(1178, 38)\n",
            "(1178,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_51 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_52 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_24 (Functional)          (None, 10)           9770        ['input_51[0][0]']               \n",
            "                                                                                                  \n",
            " model_25 (Functional)          (None, 10)           15850       ['input_52[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 20)           0           ['model_24[0][0]',               \n",
            "                                                                  'model_25[0][0]']               \n",
            "                                                                                                  \n",
            " dense_66 (Dense)               (None, 16)           336         ['concatenate_16[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "147/148 [============================>.] - ETA: 0s - loss: 1.3044(None, 16)\n",
            "148/148 [==============================] - 2s 9ms/step - loss: 1.3034 - val_loss: 1.2110\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.2140 - val_loss: 1.2101\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.2120 - val_loss: 1.2072\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.2069 - val_loss: 1.1900\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1983 - val_loss: 1.1773\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1932 - val_loss: 1.1714\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1895 - val_loss: 1.1677\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1840 - val_loss: 1.1650\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1841 - val_loss: 1.1617\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1807 - val_loss: 1.1595\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1792 - val_loss: 1.1586\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1774 - val_loss: 1.1574\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1756 - val_loss: 1.1564\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1756 - val_loss: 1.1524\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1732 - val_loss: 1.1507\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1715 - val_loss: 1.1497\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1702 - val_loss: 1.1489\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1687 - val_loss: 1.1498\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1663 - val_loss: 1.1482\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1649 - val_loss: 1.1435\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1617 - val_loss: 1.1423\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1606 - val_loss: 1.1426\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1576 - val_loss: 1.1390\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1562 - val_loss: 1.1407\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1564 - val_loss: 1.1349\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1522 - val_loss: 1.1362\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1499 - val_loss: 1.1366\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1516 - val_loss: 1.1316\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1490 - val_loss: 1.1354\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1477 - val_loss: 1.1319\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1445 - val_loss: 1.1314\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1440 - val_loss: 1.1300\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1423 - val_loss: 1.1270\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1409 - val_loss: 1.1304\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1421 - val_loss: 1.1302\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1412 - val_loss: 1.1242\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1372 - val_loss: 1.1299\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1374 - val_loss: 1.1255\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1349 - val_loss: 1.1229\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1324 - val_loss: 1.1260\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1330 - val_loss: 1.1290\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1326 - val_loss: 1.1241\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1280 - val_loss: 1.1216\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1292 - val_loss: 1.1242\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1272 - val_loss: 1.1222\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1277 - val_loss: 1.1221\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1250 - val_loss: 1.1191\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1238 - val_loss: 1.1179\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1238 - val_loss: 1.1213\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1234 - val_loss: 1.1202\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1211 - val_loss: 1.1229\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1222 - val_loss: 1.1179\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1221 - val_loss: 1.1216\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1188 - val_loss: 1.1192\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1193 - val_loss: 1.1170\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1189 - val_loss: 1.1167\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1186 - val_loss: 1.1226\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1154 - val_loss: 1.1158\n",
            "Epoch 59/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1168 - val_loss: 1.1208\n",
            "Epoch 60/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1161 - val_loss: 1.1137\n",
            "Epoch 61/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1143 - val_loss: 1.1153\n",
            "Epoch 62/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1122 - val_loss: 1.1143\n",
            "Epoch 63/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1127 - val_loss: 1.1126\n",
            "Epoch 64/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1080 - val_loss: 1.1192\n",
            "Epoch 65/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1107 - val_loss: 1.1131\n",
            "Epoch 66/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1075 - val_loss: 1.1189\n",
            "Epoch 67/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1092 - val_loss: 1.1183\n",
            "Epoch 68/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1096 - val_loss: 1.1184\n",
            "Epoch 69/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1092 - val_loss: 1.1128\n",
            "Epoch 70/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1053 - val_loss: 1.1172\n",
            "Epoch 71/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1041 - val_loss: 1.1169\n",
            "Epoch 72/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1055 - val_loss: 1.1188\n",
            "Epoch 73/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1038 - val_loss: 1.1146\n",
            "Epoch 73: early stopping\n",
            "Model: \"model_26\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_53 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_54 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_24 (Functional)          (None, 10)           9770        ['input_53[0][0]']               \n",
            "                                                                                                  \n",
            " model_25 (Functional)          (None, 10)           15850       ['input_54[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 20)           0           ['model_24[1][0]',               \n",
            "                                                                  'model_25[1][0]']               \n",
            "                                                                                                  \n",
            " dense_67 (Dense)               (None, 64)           1344        ['concatenate_17[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 64)          256         ['dense_67[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)           (None, 64)           0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dense_68 (Dense)               (None, 32)           2080        ['dropout_34[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_35 (Dropout)           (None, 32)           0           ['dense_68[0][0]']               \n",
            "                                                                                                  \n",
            " dense_69 (Dense)               (None, 16)           528         ['dropout_35[0][0]']             \n",
            "                                                                                                  \n",
            " dense_70 (Dense)               (None, 4)            68          ['dense_69[0][0]']               \n",
            "                                                                                                  \n",
            " dense_71 (Dense)               (None, 2)            10          ['dense_70[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 3s 3ms/step - loss: 0.5187 - sparse_categorical_accuracy: 0.7439 - val_loss: 0.3440 - val_sparse_categorical_accuracy: 0.8752\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.3521 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.2708 - val_sparse_categorical_accuracy: 0.8939\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.3157 - sparse_categorical_accuracy: 0.8742 - val_loss: 0.2659 - val_sparse_categorical_accuracy: 0.8973\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2921 - sparse_categorical_accuracy: 0.8856 - val_loss: 0.2647 - val_sparse_categorical_accuracy: 0.8922\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2792 - sparse_categorical_accuracy: 0.8945 - val_loss: 0.2607 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2716 - sparse_categorical_accuracy: 0.8924 - val_loss: 0.2633 - val_sparse_categorical_accuracy: 0.8939\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2754 - sparse_categorical_accuracy: 0.8897 - val_loss: 0.2617 - val_sparse_categorical_accuracy: 0.8922\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2555 - sparse_categorical_accuracy: 0.9020 - val_loss: 0.2673 - val_sparse_categorical_accuracy: 0.8956\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2586 - sparse_categorical_accuracy: 0.9013 - val_loss: 0.2610 - val_sparse_categorical_accuracy: 0.8956\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2635 - sparse_categorical_accuracy: 0.8945 - val_loss: 0.2618 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2637 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.2634 - val_sparse_categorical_accuracy: 0.8947\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2496 - sparse_categorical_accuracy: 0.9045 - val_loss: 0.2619 - val_sparse_categorical_accuracy: 0.8981\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2531 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.2626 - val_sparse_categorical_accuracy: 0.8956\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2566 - sparse_categorical_accuracy: 0.8999 - val_loss: 0.2555 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2477 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.2610 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 16/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2455 - sparse_categorical_accuracy: 0.9045 - val_loss: 0.2650 - val_sparse_categorical_accuracy: 0.8947\n",
            "Epoch 17/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2538 - sparse_categorical_accuracy: 0.9028 - val_loss: 0.2665 - val_sparse_categorical_accuracy: 0.9007\n",
            "Epoch 18/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2483 - sparse_categorical_accuracy: 0.9092 - val_loss: 0.2578 - val_sparse_categorical_accuracy: 0.8973\n",
            "Epoch 19/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2362 - sparse_categorical_accuracy: 0.9124 - val_loss: 0.2644 - val_sparse_categorical_accuracy: 0.8964\n",
            "Epoch 20/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2430 - sparse_categorical_accuracy: 0.9060 - val_loss: 0.2637 - val_sparse_categorical_accuracy: 0.8973\n",
            "Epoch 21/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2400 - sparse_categorical_accuracy: 0.9081 - val_loss: 0.2604 - val_sparse_categorical_accuracy: 0.8939\n",
            "Epoch 22/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2428 - sparse_categorical_accuracy: 0.9060 - val_loss: 0.2628 - val_sparse_categorical_accuracy: 0.8981\n",
            "Epoch 23/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2332 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.2618 - val_sparse_categorical_accuracy: 0.8947\n",
            "Epoch 24/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2330 - sparse_categorical_accuracy: 0.9119 - val_loss: 0.2577 - val_sparse_categorical_accuracy: 0.8939\n",
            "Epoch 24: early stopping\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2811 - sparse_categorical_accuracy: 0.8904\n",
            "150/150 [==============================] - 0s 1ms/step\n",
            "Test accuracy: 89.037%\n",
            "Postprocessing Test accuracy: 92.232%\n",
            "Test F1_score: 87.85%\n",
            "Postprocessing F1_score: 91.164%\n",
            "(1178, 19)\n",
            "(1178, 38)\n",
            "(1178,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_57 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_58 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_27 (Functional)          (None, 10)           9770        ['input_57[0][0]']               \n",
            "                                                                                                  \n",
            " model_28 (Functional)          (None, 10)           15850       ['input_58[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 20)           0           ['model_27[0][0]',               \n",
            "                                                                  'model_28[0][0]']               \n",
            "                                                                                                  \n",
            " dense_74 (Dense)               (None, 16)           336         ['concatenate_18[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "146/148 [============================>.] - ETA: 0s - loss: 1.2972(None, 16)\n",
            "148/148 [==============================] - 2s 6ms/step - loss: 1.2956 - val_loss: 1.2068\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.2084 - val_loss: 1.1968\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1994 - val_loss: 1.1883\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1942 - val_loss: 1.1842\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1893 - val_loss: 1.1815\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1864 - val_loss: 1.1794\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1844 - val_loss: 1.1781\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1809 - val_loss: 1.1754\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1796 - val_loss: 1.1750\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1781 - val_loss: 1.1712\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1763 - val_loss: 1.1724\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1749 - val_loss: 1.1688\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1736 - val_loss: 1.1669\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1710 - val_loss: 1.1672\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1700 - val_loss: 1.1653\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1670 - val_loss: 1.1658\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1666 - val_loss: 1.1636\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1631 - val_loss: 1.1608\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1611 - val_loss: 1.1569\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1579 - val_loss: 1.1551\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1566 - val_loss: 1.1544\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1525 - val_loss: 1.1520\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1516 - val_loss: 1.1524\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1492 - val_loss: 1.1524\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1475 - val_loss: 1.1504\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1457 - val_loss: 1.1455\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1438 - val_loss: 1.1442\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1410 - val_loss: 1.1458\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1401 - val_loss: 1.1452\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1379 - val_loss: 1.1456\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1382 - val_loss: 1.1424\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1362 - val_loss: 1.1442\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1356 - val_loss: 1.1442\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1324 - val_loss: 1.1441\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1292 - val_loss: 1.1395\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1304 - val_loss: 1.1393\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1249 - val_loss: 1.1441\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1248 - val_loss: 1.1443\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1249 - val_loss: 1.1430\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1217 - val_loss: 1.1378\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1200 - val_loss: 1.1465\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1236 - val_loss: 1.1442\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1188 - val_loss: 1.1383\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1162 - val_loss: 1.1404\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1153 - val_loss: 1.1366\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1134 - val_loss: 1.1383\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1169 - val_loss: 1.1415\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1099 - val_loss: 1.1356\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1126 - val_loss: 1.1402\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1106 - val_loss: 1.1402\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1101 - val_loss: 1.1400\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1070 - val_loss: 1.1360\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1079 - val_loss: 1.1395\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1080 - val_loss: 1.1374\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1069 - val_loss: 1.1399\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1035 - val_loss: 1.1473\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1049 - val_loss: 1.1408\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1054 - val_loss: 1.1404\n",
            "Epoch 58: early stopping\n",
            "Model: \"model_29\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_59 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_60 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_27 (Functional)          (None, 10)           9770        ['input_59[0][0]']               \n",
            "                                                                                                  \n",
            " model_28 (Functional)          (None, 10)           15850       ['input_60[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 20)           0           ['model_27[1][0]',               \n",
            "                                                                  'model_28[1][0]']               \n",
            "                                                                                                  \n",
            " dense_75 (Dense)               (None, 64)           1344        ['concatenate_19[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 64)          256         ['dense_75[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 64)           0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dense_76 (Dense)               (None, 32)           2080        ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 32)           0           ['dense_76[0][0]']               \n",
            "                                                                                                  \n",
            " dense_77 (Dense)               (None, 16)           528         ['dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            " dense_78 (Dense)               (None, 4)            68          ['dense_77[0][0]']               \n",
            "                                                                                                  \n",
            " dense_79 (Dense)               (None, 2)            10          ['dense_78[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 2s 3ms/step - loss: 0.5202 - sparse_categorical_accuracy: 0.7556 - val_loss: 0.3764 - val_sparse_categorical_accuracy: 0.8557\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.3341 - sparse_categorical_accuracy: 0.8648 - val_loss: 0.3260 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2915 - sparse_categorical_accuracy: 0.8882 - val_loss: 0.3225 - val_sparse_categorical_accuracy: 0.8718\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2883 - sparse_categorical_accuracy: 0.8856 - val_loss: 0.3143 - val_sparse_categorical_accuracy: 0.8727\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2728 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.3170 - val_sparse_categorical_accuracy: 0.8769\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2745 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.3140 - val_sparse_categorical_accuracy: 0.8718\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2681 - sparse_categorical_accuracy: 0.8969 - val_loss: 0.3117 - val_sparse_categorical_accuracy: 0.8795\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2668 - sparse_categorical_accuracy: 0.8967 - val_loss: 0.3101 - val_sparse_categorical_accuracy: 0.8769\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2522 - sparse_categorical_accuracy: 0.8986 - val_loss: 0.3162 - val_sparse_categorical_accuracy: 0.8744\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2678 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.3097 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2545 - sparse_categorical_accuracy: 0.9011 - val_loss: 0.3092 - val_sparse_categorical_accuracy: 0.8752\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2469 - sparse_categorical_accuracy: 0.9081 - val_loss: 0.3136 - val_sparse_categorical_accuracy: 0.8684\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2506 - sparse_categorical_accuracy: 0.9028 - val_loss: 0.3110 - val_sparse_categorical_accuracy: 0.8727\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2425 - sparse_categorical_accuracy: 0.9079 - val_loss: 0.3192 - val_sparse_categorical_accuracy: 0.8718\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2329 - sparse_categorical_accuracy: 0.9105 - val_loss: 0.3181 - val_sparse_categorical_accuracy: 0.8727\n",
            "Epoch 16/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2383 - sparse_categorical_accuracy: 0.9088 - val_loss: 0.3253 - val_sparse_categorical_accuracy: 0.8710\n",
            "Epoch 17/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2413 - sparse_categorical_accuracy: 0.9081 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.8727\n",
            "Epoch 18/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2388 - sparse_categorical_accuracy: 0.9088 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.8701\n",
            "Epoch 19/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2364 - sparse_categorical_accuracy: 0.9107 - val_loss: 0.3258 - val_sparse_categorical_accuracy: 0.8735\n",
            "Epoch 20/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2374 - sparse_categorical_accuracy: 0.9090 - val_loss: 0.3188 - val_sparse_categorical_accuracy: 0.8676\n",
            "Epoch 21/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2376 - sparse_categorical_accuracy: 0.9094 - val_loss: 0.3178 - val_sparse_categorical_accuracy: 0.8710\n",
            "Epoch 21: early stopping\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2828 - sparse_categorical_accuracy: 0.8875\n",
            "150/150 [==============================] - 0s 1ms/step\n",
            "Test accuracy: 88.745%\n",
            "Postprocessing Test accuracy: 92.566%\n",
            "Test F1_score: 87.404%\n",
            "Postprocessing F1_score: 91.544%\n",
            "(1178, 19)\n",
            "(1178, 38)\n",
            "(1178,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_63 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_64 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_30 (Functional)          (None, 10)           9770        ['input_63[0][0]']               \n",
            "                                                                                                  \n",
            " model_31 (Functional)          (None, 10)           15850       ['input_64[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 20)           0           ['model_30[0][0]',               \n",
            "                                                                  'model_31[0][0]']               \n",
            "                                                                                                  \n",
            " dense_82 (Dense)               (None, 16)           336         ['concatenate_20[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "146/148 [============================>.] - ETA: 0s - loss: 1.3212(None, 16)\n",
            "148/148 [==============================] - 2s 6ms/step - loss: 1.3195 - val_loss: 1.2104\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.2129 - val_loss: 1.2073\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.2067 - val_loss: 1.1985\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1980 - val_loss: 1.1913\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1899 - val_loss: 1.1876\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1877 - val_loss: 1.1836\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1821 - val_loss: 1.1797\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1792 - val_loss: 1.1792\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1797 - val_loss: 1.1757\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1763 - val_loss: 1.1733\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1736 - val_loss: 1.1731\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1724 - val_loss: 1.1737\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1719 - val_loss: 1.1704\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1679 - val_loss: 1.1704\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1659 - val_loss: 1.1637\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1624 - val_loss: 1.1640\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1614 - val_loss: 1.1593\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1581 - val_loss: 1.1617\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1549 - val_loss: 1.1569\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1535 - val_loss: 1.1523\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1516 - val_loss: 1.1512\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1481 - val_loss: 1.1598\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1464 - val_loss: 1.1526\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1461 - val_loss: 1.1460\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1418 - val_loss: 1.1516\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1409 - val_loss: 1.1440\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1411 - val_loss: 1.1482\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1375 - val_loss: 1.1418\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1363 - val_loss: 1.1409\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1318 - val_loss: 1.1419\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1340 - val_loss: 1.1389\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1309 - val_loss: 1.1378\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1298 - val_loss: 1.1354\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1277 - val_loss: 1.1341\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1296 - val_loss: 1.1337\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1223 - val_loss: 1.1353\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1229 - val_loss: 1.1321\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1205 - val_loss: 1.1292\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1205 - val_loss: 1.1332\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1191 - val_loss: 1.1362\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1180 - val_loss: 1.1401\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1157 - val_loss: 1.1385\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1152 - val_loss: 1.1309\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1154 - val_loss: 1.1318\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1156 - val_loss: 1.1287\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1099 - val_loss: 1.1262\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1090 - val_loss: 1.1328\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1072 - val_loss: 1.1345\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1081 - val_loss: 1.1235\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1070 - val_loss: 1.1283\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1069 - val_loss: 1.1244\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1048 - val_loss: 1.1312\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1079 - val_loss: 1.1209\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1054 - val_loss: 1.1256\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1042 - val_loss: 1.1237\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1035 - val_loss: 1.1262\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1026 - val_loss: 1.1241\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1015 - val_loss: 1.1279\n",
            "Epoch 59/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1004 - val_loss: 1.1239\n",
            "Epoch 60/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0959 - val_loss: 1.1266\n",
            "Epoch 61/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0988 - val_loss: 1.1269\n",
            "Epoch 62/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0975 - val_loss: 1.1308\n",
            "Epoch 63/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0948 - val_loss: 1.1281\n",
            "Epoch 63: early stopping\n",
            "Model: \"model_32\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_65 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_66 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_30 (Functional)          (None, 10)           9770        ['input_65[0][0]']               \n",
            "                                                                                                  \n",
            " model_31 (Functional)          (None, 10)           15850       ['input_66[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 20)           0           ['model_30[1][0]',               \n",
            "                                                                  'model_31[1][0]']               \n",
            "                                                                                                  \n",
            " dense_83 (Dense)               (None, 64)           1344        ['concatenate_21[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 64)          256         ['dense_83[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 64)           0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 32)           2080        ['dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 32)           0           ['dense_84[0][0]']               \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (None, 16)           528         ['dropout_43[0][0]']             \n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (None, 4)            68          ['dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 2)            10          ['dense_86[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 2s 3ms/step - loss: 0.5998 - sparse_categorical_accuracy: 0.7161 - val_loss: 0.4717 - val_sparse_categorical_accuracy: 0.8200\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.3602 - sparse_categorical_accuracy: 0.8527 - val_loss: 0.2872 - val_sparse_categorical_accuracy: 0.8862\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.3126 - sparse_categorical_accuracy: 0.8769 - val_loss: 0.2800 - val_sparse_categorical_accuracy: 0.8896\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2785 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.2757 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2728 - sparse_categorical_accuracy: 0.8967 - val_loss: 0.2694 - val_sparse_categorical_accuracy: 0.8930\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2473 - sparse_categorical_accuracy: 0.9060 - val_loss: 0.2732 - val_sparse_categorical_accuracy: 0.8888\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2485 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.2713 - val_sparse_categorical_accuracy: 0.8922\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2477 - sparse_categorical_accuracy: 0.9037 - val_loss: 0.2845 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2405 - sparse_categorical_accuracy: 0.9086 - val_loss: 0.2755 - val_sparse_categorical_accuracy: 0.8871\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2496 - sparse_categorical_accuracy: 0.9058 - val_loss: 0.2784 - val_sparse_categorical_accuracy: 0.8905\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2366 - sparse_categorical_accuracy: 0.9102 - val_loss: 0.2823 - val_sparse_categorical_accuracy: 0.8905\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2405 - sparse_categorical_accuracy: 0.9096 - val_loss: 0.2773 - val_sparse_categorical_accuracy: 0.8896\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2364 - sparse_categorical_accuracy: 0.9088 - val_loss: 0.2770 - val_sparse_categorical_accuracy: 0.8939\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2287 - sparse_categorical_accuracy: 0.9181 - val_loss: 0.2776 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9113 - val_loss: 0.2714 - val_sparse_categorical_accuracy: 0.8939\n",
            "Epoch 15: early stopping\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2792 - sparse_categorical_accuracy: 0.8900\n",
            "150/150 [==============================] - 0s 1ms/step\n",
            "Test accuracy: 88.996%\n",
            "Postprocessing Test accuracy: 92.274%\n",
            "Test F1_score: 87.591%\n",
            "Postprocessing F1_score: 91.211%\n",
            "(1178, 19)\n",
            "(1178, 38)\n",
            "(1178,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_69 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_70 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_33 (Functional)          (None, 10)           9770        ['input_69[0][0]']               \n",
            "                                                                                                  \n",
            " model_34 (Functional)          (None, 10)           15850       ['input_70[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 20)           0           ['model_33[0][0]',               \n",
            "                                                                  'model_34[0][0]']               \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 16)           336         ['concatenate_22[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "148/148 [==============================] - ETA: 0s - loss: 1.3241(None, 16)\n",
            "148/148 [==============================] - 2s 5ms/step - loss: 1.3241 - val_loss: 1.2079\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.2099 - val_loss: 1.1977\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.2014 - val_loss: 1.1903\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1941 - val_loss: 1.1875\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1904 - val_loss: 1.1833\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1861 - val_loss: 1.1819\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1839 - val_loss: 1.1811\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1837 - val_loss: 1.1767\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1817 - val_loss: 1.1752\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1766 - val_loss: 1.1744\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1758 - val_loss: 1.1753\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1754 - val_loss: 1.1706\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1740 - val_loss: 1.1685\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1700 - val_loss: 1.1657\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1693 - val_loss: 1.1649\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1647 - val_loss: 1.1646\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1652 - val_loss: 1.1614\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1633 - val_loss: 1.1604\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1620 - val_loss: 1.1582\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1604 - val_loss: 1.1586\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1588 - val_loss: 1.1553\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1579 - val_loss: 1.1573\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1548 - val_loss: 1.1566\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1523 - val_loss: 1.1542\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1552 - val_loss: 1.1499\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1503 - val_loss: 1.1523\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1489 - val_loss: 1.1496\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1459 - val_loss: 1.1490\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1471 - val_loss: 1.1477\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1430 - val_loss: 1.1454\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1417 - val_loss: 1.1449\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1404 - val_loss: 1.1439\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1381 - val_loss: 1.1467\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1362 - val_loss: 1.1464\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1336 - val_loss: 1.1431\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1346 - val_loss: 1.1435\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1314 - val_loss: 1.1389\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1309 - val_loss: 1.1399\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1301 - val_loss: 1.1402\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1268 - val_loss: 1.1415\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1268 - val_loss: 1.1398\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1248 - val_loss: 1.1459\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1245 - val_loss: 1.1349\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1235 - val_loss: 1.1345\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1216 - val_loss: 1.1364\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1210 - val_loss: 1.1340\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1215 - val_loss: 1.1366\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1180 - val_loss: 1.1365\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1149 - val_loss: 1.1377\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1156 - val_loss: 1.1335\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1152 - val_loss: 1.1368\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1138 - val_loss: 1.1383\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1123 - val_loss: 1.1357\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1123 - val_loss: 1.1323\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1095 - val_loss: 1.1310\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1091 - val_loss: 1.1369\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1099 - val_loss: 1.1306\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1098 - val_loss: 1.1300\n",
            "Epoch 59/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1059 - val_loss: 1.1295\n",
            "Epoch 60/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1060 - val_loss: 1.1291\n",
            "Epoch 61/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1015 - val_loss: 1.1298\n",
            "Epoch 62/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1006 - val_loss: 1.1283\n",
            "Epoch 63/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1053 - val_loss: 1.1354\n",
            "Epoch 64/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1031 - val_loss: 1.1278\n",
            "Epoch 65/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0984 - val_loss: 1.1315\n",
            "Epoch 66/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0978 - val_loss: 1.1321\n",
            "Epoch 67/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0979 - val_loss: 1.1263\n",
            "Epoch 68/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0959 - val_loss: 1.1330\n",
            "Epoch 69/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0939 - val_loss: 1.1358\n",
            "Epoch 70/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0950 - val_loss: 1.1302\n",
            "Epoch 71/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0962 - val_loss: 1.1330\n",
            "Epoch 72/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0927 - val_loss: 1.1307\n",
            "Epoch 73/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0926 - val_loss: 1.1349\n",
            "Epoch 74/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0934 - val_loss: 1.1338\n",
            "Epoch 75/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0887 - val_loss: 1.1321\n",
            "Epoch 76/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0883 - val_loss: 1.1268\n",
            "Epoch 77/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0888 - val_loss: 1.1275\n",
            "Epoch 77: early stopping\n",
            "Model: \"model_35\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_71 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_72 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_33 (Functional)          (None, 10)           9770        ['input_71[0][0]']               \n",
            "                                                                                                  \n",
            " model_34 (Functional)          (None, 10)           15850       ['input_72[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 20)           0           ['model_33[1][0]',               \n",
            "                                                                  'model_34[1][0]']               \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 64)           1344        ['concatenate_23[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 64)          256         ['dense_91[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)           (None, 64)           0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 32)           2080        ['dropout_46[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 32)           0           ['dense_92[0][0]']               \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 16)           528         ['dropout_47[0][0]']             \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 4)            68          ['dense_93[0][0]']               \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 2)            10          ['dense_94[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 2s 3ms/step - loss: 0.5657 - sparse_categorical_accuracy: 0.7369 - val_loss: 0.4404 - val_sparse_categorical_accuracy: 0.8480\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.3461 - sparse_categorical_accuracy: 0.8642 - val_loss: 0.3073 - val_sparse_categorical_accuracy: 0.8744\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2848 - sparse_categorical_accuracy: 0.8916 - val_loss: 0.2891 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2491 - sparse_categorical_accuracy: 0.9056 - val_loss: 0.2976 - val_sparse_categorical_accuracy: 0.8803\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2418 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.2907 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2348 - sparse_categorical_accuracy: 0.9143 - val_loss: 0.2945 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2271 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.2888 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2178 - sparse_categorical_accuracy: 0.9156 - val_loss: 0.2873 - val_sparse_categorical_accuracy: 0.8820\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2182 - sparse_categorical_accuracy: 0.9204 - val_loss: 0.2896 - val_sparse_categorical_accuracy: 0.8803\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2296 - sparse_categorical_accuracy: 0.9128 - val_loss: 0.2901 - val_sparse_categorical_accuracy: 0.8803\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2088 - sparse_categorical_accuracy: 0.9221 - val_loss: 0.2916 - val_sparse_categorical_accuracy: 0.8820\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2262 - sparse_categorical_accuracy: 0.9160 - val_loss: 0.2838 - val_sparse_categorical_accuracy: 0.8769\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2101 - sparse_categorical_accuracy: 0.9247 - val_loss: 0.2865 - val_sparse_categorical_accuracy: 0.8795\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2161 - sparse_categorical_accuracy: 0.9198 - val_loss: 0.2879 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2127 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.2866 - val_sparse_categorical_accuracy: 0.8829\n",
            "Epoch 16/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2002 - sparse_categorical_accuracy: 0.9270 - val_loss: 0.2977 - val_sparse_categorical_accuracy: 0.8820\n",
            "Epoch 17/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.1962 - sparse_categorical_accuracy: 0.9257 - val_loss: 0.2929 - val_sparse_categorical_accuracy: 0.8837\n",
            "Epoch 18/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.1989 - sparse_categorical_accuracy: 0.9304 - val_loss: 0.2886 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 19/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9272 - val_loss: 0.2967 - val_sparse_categorical_accuracy: 0.8803\n",
            "Epoch 20/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2021 - sparse_categorical_accuracy: 0.9262 - val_loss: 0.2883 - val_sparse_categorical_accuracy: 0.8803\n",
            "Epoch 21/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2035 - sparse_categorical_accuracy: 0.9223 - val_loss: 0.2877 - val_sparse_categorical_accuracy: 0.8786\n",
            "Epoch 22/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2035 - sparse_categorical_accuracy: 0.9204 - val_loss: 0.2870 - val_sparse_categorical_accuracy: 0.8846\n",
            "Epoch 22: early stopping\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3008 - sparse_categorical_accuracy: 0.8843\n",
            "150/150 [==============================] - 0s 1ms/step\n",
            "Test accuracy: 88.432%\n",
            "Postprocessing Test accuracy: 92.19%\n",
            "Test F1_score: 86.897%\n",
            "Postprocessing F1_score: 91.116%\n",
            "(1179, 19)\n",
            "(1179, 38)\n",
            "(1179,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_75 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_76 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_36 (Functional)          (None, 10)           9770        ['input_75[0][0]']               \n",
            "                                                                                                  \n",
            " model_37 (Functional)          (None, 10)           15850       ['input_76[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 20)           0           ['model_36[0][0]',               \n",
            "                                                                  'model_37[0][0]']               \n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 16)           336         ['concatenate_24[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "135/148 [==========================>...] - ETA: 0s - loss: 1.3212(None, 16)\n",
            "148/148 [==============================] - 2s 8ms/step - loss: 1.3113 - val_loss: 1.2109\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.2121 - val_loss: 1.2106\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.2110 - val_loss: 1.2067\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.2034 - val_loss: 1.1920\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1944 - val_loss: 1.1873\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1899 - val_loss: 1.1842\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1874 - val_loss: 1.1810\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1839 - val_loss: 1.1791\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1816 - val_loss: 1.1764\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1807 - val_loss: 1.1754\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1770 - val_loss: 1.1739\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1754 - val_loss: 1.1715\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1746 - val_loss: 1.1695\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1730 - val_loss: 1.1663\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1686 - val_loss: 1.1657\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1645 - val_loss: 1.1646\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1622 - val_loss: 1.1585\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1601 - val_loss: 1.1587\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1564 - val_loss: 1.1590\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1551 - val_loss: 1.1615\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1516 - val_loss: 1.1536\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1505 - val_loss: 1.1536\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1437 - val_loss: 1.1594\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1454 - val_loss: 1.1515\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1410 - val_loss: 1.1501\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1410 - val_loss: 1.1514\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1372 - val_loss: 1.1564\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1361 - val_loss: 1.1551\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1365 - val_loss: 1.1468\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1335 - val_loss: 1.1507\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1325 - val_loss: 1.1459\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1314 - val_loss: 1.1507\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1297 - val_loss: 1.1548\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1267 - val_loss: 1.1477\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1263 - val_loss: 1.1456\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1242 - val_loss: 1.1483\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1263 - val_loss: 1.1517\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1225 - val_loss: 1.1465\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1202 - val_loss: 1.1516\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1210 - val_loss: 1.1540\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1202 - val_loss: 1.1459\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1194 - val_loss: 1.1452\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1175 - val_loss: 1.1506\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1185 - val_loss: 1.1479\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1178 - val_loss: 1.1461\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1149 - val_loss: 1.1466\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1126 - val_loss: 1.1522\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1143 - val_loss: 1.1454\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1143 - val_loss: 1.1451\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1111 - val_loss: 1.1509\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1079 - val_loss: 1.1536\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1092 - val_loss: 1.1547\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1085 - val_loss: 1.1503\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1100 - val_loss: 1.1487\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1078 - val_loss: 1.1524\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1054 - val_loss: 1.1484\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1084 - val_loss: 1.1522\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1090 - val_loss: 1.1491\n",
            "Epoch 59/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1047 - val_loss: 1.1513\n",
            "Epoch 59: early stopping\n",
            "Model: \"model_38\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_77 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_78 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_36 (Functional)          (None, 10)           9770        ['input_77[0][0]']               \n",
            "                                                                                                  \n",
            " model_37 (Functional)          (None, 10)           15850       ['input_78[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, 20)           0           ['model_36[1][0]',               \n",
            "                                                                  'model_37[1][0]']               \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 64)           1344        ['concatenate_25[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 64)          256         ['dense_99[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 64)           0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dense_100 (Dense)              (None, 32)           2080        ['dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 32)           0           ['dense_100[0][0]']              \n",
            "                                                                                                  \n",
            " dense_101 (Dense)              (None, 16)           528         ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            " dense_102 (Dense)              (None, 4)            68          ['dense_101[0][0]']              \n",
            "                                                                                                  \n",
            " dense_103 (Dense)              (None, 2)            10          ['dense_102[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 3s 3ms/step - loss: 0.4468 - sparse_categorical_accuracy: 0.7990 - val_loss: 0.3533 - val_sparse_categorical_accuracy: 0.8567\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.3200 - sparse_categorical_accuracy: 0.8735 - val_loss: 0.3543 - val_sparse_categorical_accuracy: 0.8617\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2853 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.3650 - val_sparse_categorical_accuracy: 0.8601\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2806 - sparse_categorical_accuracy: 0.8901 - val_loss: 0.3594 - val_sparse_categorical_accuracy: 0.8626\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2823 - sparse_categorical_accuracy: 0.8933 - val_loss: 0.3487 - val_sparse_categorical_accuracy: 0.8601\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2704 - sparse_categorical_accuracy: 0.8973 - val_loss: 0.3454 - val_sparse_categorical_accuracy: 0.8609\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2620 - sparse_categorical_accuracy: 0.8994 - val_loss: 0.3483 - val_sparse_categorical_accuracy: 0.8617\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2714 - sparse_categorical_accuracy: 0.8920 - val_loss: 0.3595 - val_sparse_categorical_accuracy: 0.8626\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2560 - sparse_categorical_accuracy: 0.9045 - val_loss: 0.3544 - val_sparse_categorical_accuracy: 0.8626\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2494 - sparse_categorical_accuracy: 0.9009 - val_loss: 0.3600 - val_sparse_categorical_accuracy: 0.8609\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2481 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.3475 - val_sparse_categorical_accuracy: 0.8617\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2500 - sparse_categorical_accuracy: 0.9034 - val_loss: 0.3554 - val_sparse_categorical_accuracy: 0.8634\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2436 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.3667 - val_sparse_categorical_accuracy: 0.8626\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2479 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.3500 - val_sparse_categorical_accuracy: 0.8609\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 0.2403 - sparse_categorical_accuracy: 0.9115 - val_loss: 0.3557 - val_sparse_categorical_accuracy: 0.8609\n",
            "Epoch 16/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2479 - sparse_categorical_accuracy: 0.9043 - val_loss: 0.3546 - val_sparse_categorical_accuracy: 0.8634\n",
            "Epoch 16: early stopping\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2979 - sparse_categorical_accuracy: 0.8875\n",
            "150/150 [==============================] - 0s 1ms/step\n",
            "Test accuracy: 88.745%\n",
            "Postprocessing Test accuracy: 92.316%\n",
            "Test F1_score: 87.267%\n",
            "Postprocessing F1_score: 91.259%\n"
          ]
        }
      ],
      "source": [
        "for j in range(0,5): # folds \n",
        "\n",
        "  xval_a = xtrain[(train_size*j)//5:(train_size*(j+1))//5];\n",
        "  print(xval_a.shape)\n",
        "  yval_a = ytrain[(train_size*j)//5:(train_size*(j+1))//5]\n",
        "  xtra_a = np.concatenate((xtrain[:(train_size*j)//5], xtrain[((train_size*(j+1))//5):]), axis=0) \n",
        "  ytra_a = np.concatenate((ytrain[:(train_size*j)//5], ytrain[((train_size*(j+1))//5):]), axis=0)\n",
        "  # print('Fold  '+str(j+1)+'  xtrain shape:::::::', xtra_a.shape)\n",
        "\n",
        "  xval_ac = xtrain1[(train_size*j)//5:(train_size*(j+1))//5];\n",
        "  print(xval_ac.shape)\n",
        "  yval_ac = ytrain1[(train_size*j)//5:(train_size*(j+1))//5]\n",
        "  xtra_ac = np.concatenate((xtrain1[:(train_size*j)//5], xtrain1[((train_size*(j+1))//5):]), axis=0)\n",
        "  ytra_ac = np.concatenate((ytrain1[:(train_size*j)//5], ytrain1[((train_size*(j+1))//5):]), axis=0)\n",
        "  y = np.concatenate((ytra_a,ytra_a), axis=0)\n",
        "  yv = np.concatenate((yval_a,yval_a), axis=0)\n",
        "  print(yval_ac.shape)\n",
        "  #check what's happening here is it intended\n",
        "  encoder1 = create_encoder1(latent_dim=10)\n",
        "  encoder2 = create_encoder2(10)\n",
        "  encoder_with_projection_head = add_projection_head1(encoder1,encoder2)\n",
        "  encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),loss=SupervisedContrastiveLoss(temperature))\n",
        "  encoder_with_projection_head.summary()\n",
        "                                                            #ytra_a                                  #yval_a\n",
        "  history = encoder_with_projection_head.fit([xtra_a,xtra_ac], ytra_ac , validation_data =([xval_a,xval_ac],yval_ac), batch_size=32, epochs=100, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
        "\n",
        "\n",
        "  learning_rate = 0.0005\n",
        "  batch_size = 16\n",
        "  hidden_units = 64\n",
        "  projection_units = 128\n",
        "  num_epochs = 200\n",
        "  dropout_rate = 0.3\n",
        "  num_classes = 2\n",
        "  input_shape1 = (19,)\n",
        "  input_shape2 = (38,)\n",
        "\n",
        "  from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
        "  classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
        "  classifier.summary()\n",
        "  history = classifier.fit(x=[xtra_a,xtra_ac], y=ytra_ac, validation_data =([xval_a,xval_ac],yval_ac), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
        "\n",
        "  accuracy = classifier.evaluate([xtest_a,xtest_ac], ytest)[1]\n",
        "\n",
        "  ##  Accuracy on post processed labels (each word should contain only one stressed syllable)\n",
        "  pred_output= classifier.predict([xtest_a,xtest_ac])\n",
        "  # pred_labels= pred_output.argmax(axis =1)\n",
        "  pred1_labels = pred_output[:,1]\n",
        "  post_labels = make_partitions(wtest, pred1_labels)\n",
        "  post_accuracy = calculate_accuracy(post_labels, ytest)\n",
        "\n",
        "  F1_score_WoPP = f1_score(ytest, pred_output.argmax(axis =1))\n",
        "  F1_score_WPP = f1_score(ytest, post_labels)\n",
        "\n",
        "  print(f\"Test accuracy: {round(accuracy * 100, 3)}%\")\n",
        "  print(f\"Postprocessing Test accuracy: {round(post_accuracy * 100, 3)}%\")\n",
        "  print(f\"Test F1_score: {round(F1_score_WoPP * 100, 3)}%\")\n",
        "  print(f\"Postprocessing F1_score: {round(F1_score_WPP * 100, 3)}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
