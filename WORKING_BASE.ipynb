{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Collecting pytz>=2020.1\n",
            "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "Collecting tzdata>=2022.1\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Collecting numpy>=1.21.0\n",
            "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Using cached contourpy-1.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Using cached fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Using cached kiwisolver-1.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (23.0)\n",
            "Collecting pillow>=6.2.0\n",
            "  Using cached Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "Collecting pyparsing>=2.3.1\n",
            "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Collecting scipy>=1.3.2\n",
            "  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting absl-py>=1.0.0\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Using cached flatbuffers-23.5.9-py2.py3-none-any.whl (26 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Using cached grpcio-1.55.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Using cached h5py-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Collecting jax>=0.3.15\n",
            "  Using cached jax-0.4.10-py3-none-any.whl\n",
            "Collecting keras<2.13,>=2.12.0\n",
            "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Collecting libclang>=13.0.0\n",
            "  Using cached libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
            "Collecting numpy>=1.21.0\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Using cached protobuf-4.23.1-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "Requirement already satisfied: setuptools in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (67.6.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.16.0)\n",
            "Collecting tensorboard<2.13,>=2.12\n",
            "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
            "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (4.5.0)\n",
            "Collecting wrapt<1.15,>=1.11.0\n",
            "  Using cached wrapt-1.14.1-cp311-cp311-linux_x86_64.whl\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 4)) (0.40.0)\n",
            "Collecting ml-dtypes>=0.1.0\n",
            "  Using cached ml_dtypes-0.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Using cached google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (2.28.2)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Using cached tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Using cached Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "Requirement already satisfied: urllib3<2.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (1.26.15)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (2.1.2)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6\n",
            "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Installing collected packages: pytz, libclang, flatbuffers, wrapt, werkzeug, tzdata, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyparsing, pyasn1, protobuf, pillow, oauthlib, numpy, markdown, kiwisolver, keras, joblib, grpcio, google-pasta, gast, fonttools, cycler, cachetools, astunparse, absl-py, scipy, rsa, requests-oauthlib, pyasn1-modules, pandas, opt-einsum, ml-dtypes, h5py, contourpy, scikit-learn, matplotlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 contourpy-1.0.7 cycler-0.11.0 flatbuffers-23.5.9 fonttools-4.39.4 gast-0.4.0 google-auth-2.18.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.55.0 h5py-3.8.0 jax-0.4.10 joblib-1.2.0 keras-2.12.0 kiwisolver-1.4.4 libclang-16.0.0 markdown-3.4.3 matplotlib-3.7.1 ml-dtypes-0.1.0 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-2.0.1 pillow-9.5.0 protobuf-4.23.1 pyasn1-0.5.0 pyasn1-modules-0.3.0 pyparsing-3.0.9 pytz-2023.3 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.2.2 scipy-1.10.1 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 threadpoolctl-3.1.0 tzdata-2023.3 werkzeug-2.3.4 wrapt-1.14.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hIgjuMjCEz4",
        "outputId": "907d00f0-d421-4566-99a9-d3334df7b87e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-23 11:21:49.074663: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-05-23 11:21:49.181174: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-05-23 11:21:49.181873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-23 11:21:50.499233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import backend as K\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Reshape\n",
        "temperature = 0.03\n",
        "learning_rate=0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dataloader(path, featType):\n",
        "    \"\"\"\n",
        "    Load data from a MATLAB file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the MATLAB file.\n",
        "        featType (int): Type of features to load.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
        "    \"\"\"\n",
        "    data = scipy.io.loadmat(path)\n",
        "    print(data.keys())\n",
        "\n",
        "    AF = data['AF']\n",
        "    x1 = AF[:-2]\n",
        "    y = AF[-2]\n",
        "    w = AF[-1]\n",
        "\n",
        "    if featType == 1:\n",
        "        x = x1\n",
        "    else:\n",
        "        x2 = data['CF']\n",
        "        x = np.concatenate((x1, x2), axis=0)\n",
        "    return x.T, y.T, w.T, data['CF_info']\n",
        "\n",
        "def calculate_accuracy(arr1, arr2):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy between two arrays.\n",
        "\n",
        "    Args:\n",
        "        arr1 (array): First array.\n",
        "        arr2 (array): Second array.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy between the two arrays.\n",
        "    \"\"\"\n",
        "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
        "    return count / len(arr1)\n",
        "\n",
        "def normalization(feats):\n",
        "\n",
        "    \"\"\"\n",
        "    Normalize the input features using standard scaling.\n",
        "\n",
        "    Args:\n",
        "        feats (array): Input features.\n",
        "\n",
        "    Returns:\n",
        "        array: Normalized features.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(feats)\n",
        "    scaler = StandardScaler()\n",
        "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "    return x_new\n",
        "\n",
        "def make_partitions(arr_words, arr_labels):\n",
        "\n",
        "    \"\"\"\n",
        "    Create partitions based on word boundaries and labels.\n",
        "\n",
        "    Args:\n",
        "        arr_words (array): Array of words.\n",
        "        arr_labels (array): Array of labels.\n",
        "\n",
        "    Returns:\n",
        "        array: Partitions based on word boundaries and labels.\n",
        "    \"\"\"\n",
        "    v = []\n",
        "    temp = []\n",
        "\n",
        "    for i in range(len(arr_words) - 1):\n",
        "        word = arr_words[i]\n",
        "        next_word = arr_words[i + 1]\n",
        "        temp.append(arr_labels[i])\n",
        "\n",
        "        if word != next_word or i == len(arr_words) - 2:\n",
        "            if i == len(arr_words) - 2:\n",
        "                temp.append(arr_labels[i + 1])\n",
        "\n",
        "            numpy_temp = np.array(temp)\n",
        "            temp_max = np.amax(numpy_temp)\n",
        "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
        "            v = np.concatenate((v, numpy_temp), axis=None)\n",
        "            temp.clear()\n",
        "\n",
        "    v1 = [1 if i == 1 else 0 for i in v]\n",
        "    return v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pl9qUu5yUZ7S"
      },
      "outputs": [],
      "source": [
        "fatyp = 'TypicalFA_comb1'\n",
        "drivepath = 'finalData/'+ fatyp +'/';\n",
        "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
        "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
        "featType = 1; #Acoustic or Acoustic+context\n",
        "if featType == 1:\n",
        "  original_dim = 19\n",
        "else:\n",
        "  original_dim = 38"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHXtU-QbUgV3",
        "outputId": "72d487df-eb40-47ed-c809-034c63253bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
          ]
        }
      ],
      "source": [
        "# print('Classification with::::::',os.path.basename(filee))\n",
        "\n",
        "train_path = filee; test_path = filee.replace('train','test')\n",
        "# print('test file:::::::',os.path.basename(test_path))\n",
        "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
        "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
        "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
        "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_c6qOmVsZZJ9"
      },
      "outputs": [],
      "source": [
        "xtest_a = normalization(xtest)\n",
        "xtest_ac = normalization(xtest1)\n",
        "xtrain = normalization(xtrain)\n",
        "xtrain1 = normalization(xtrain1)\n",
        "\n",
        "woPP=[]; wPP=[]\n",
        "input_shape1 = (19,1)\n",
        "input_shape2 = (38,1)\n",
        "temperature = 0.03\n",
        "learning_rate=0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=temperature, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        print(feature_vectors.shape)\n",
        "        # labels = tf.keras.layers.Concatenate(axis=0)([labels, labels])\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = tf.divide(tf.matmul(feature_vectors_normalized, tf.transpose(feature_vectors_normalized)), self.temperature)\n",
        "        \n",
        "        # print(feature_vectors.shape)\n",
        "        # print(labels.shape)\n",
        "        # print('loss:::::::', tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
        "        return 0.35*(tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
        "        #find out more about why 0.35 is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5zf__kwAZuU3"
      },
      "outputs": [],
      "source": [
        "latent_dim = 30\n",
        "\n",
        "class Encoder1(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Encoder1, self).__init__()\n",
        "    self.latent_dim = latent_dim \n",
        "    inputs = Input(shape=(19,1))\n",
        "    outputs = inputs  \n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      inputs,\n",
        "      \n",
        "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
        "      layers.MaxPooling1D(pool_size=1),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
        "      layers.MaxPooling1D(pool_size=1),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(latent_dim, activation='relu'),\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    return encoded\n",
        "\n",
        "class Encoder2(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Encoder2, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    inputs = Input(shape=(38,1))\n",
        "    outputs = inputs  \n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      inputs,\n",
        "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
        "      layers.MaxPooling1D(pool_size=1),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
        "      layers.MaxPooling1D(pool_size=1),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(latent_dim, activation='relu'),\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_jEHY9QfZyxH"
      },
      "outputs": [],
      "source": [
        "def create_encoder1():\n",
        "    return Encoder1(latent_dim)\n",
        "\n",
        "def create_encoder2():\n",
        "    return Encoder2(latent_dim)\n",
        "\n",
        "def add_projection_head1(Encoder1, Encoder2):\n",
        "    inp1 = keras.Input(shape=input_shape1)\n",
        "    inp2 = keras.Input(shape=input_shape2)\n",
        "    hidden3a  = Encoder1(inp1)\n",
        "    hidden3b = Encoder2(inp2)\n",
        "    features = tf.keras.layers.Concatenate(axis=1)([hidden3a, hidden3b])\n",
        "    features = layers.Dense(16, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=[inp1, inp2], outputs=features, name=\"cifar-encoder_with_projection-head\"\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xSS1tHvAZ4AA"
      },
      "outputs": [],
      "source": [
        "def create_classifier(encoder, trainable):\n",
        "\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    inputs1 = keras.Input(shape=input_shape1)\n",
        "    inputs2 = keras.Input(shape=input_shape2)\n",
        "    features1 = encoder1(inputs1)\n",
        "    features2 = encoder2(inputs2)\n",
        "    features = tf.keras.layers.Concatenate(axis=1)([features1, features2])\n",
        "    # features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
        "    \n",
        "    features = layers.BatchNormalization()(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(32, activation=\"relu\")(features)\n",
        "    features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(16, activation=\"relu\")(features)\n",
        "    # features = layers.BatchNormalization()(features)\n",
        "    # features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(4, activation=\"relu\")(features)\n",
        "    # features = layers.BatchNormalization()(features)\n",
        "    # features = layers.Dropout(0.1)(features)\n",
        "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(features)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gClzPzIzmj6L"
      },
      "source": [
        "#single iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gtjp_waVryHJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "# Splitting xtrain and ytrain into training and validation sets\n",
        "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
        "\n",
        "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
        "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUWyLi4arA7L",
        "outputId": "949840b8-ddee-41e6-a948-724edc943e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 19, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 38, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " encoder1 (Encoder1)            (None, 30)           20670       ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " encoder2 (Encoder2)            (None, 30)           38910       ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 60)           0           ['encoder1[0][0]',               \n",
            "                                                                  'encoder2[0][0]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 16)           976         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,556\n",
            "Trainable params: 60,556\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "137/148 [==========================>...] - ETA: 0s - loss: 1.2335(None, 16)\n",
            "148/148 [==============================] - 3s 10ms/step - loss: 1.2299 - val_loss: 1.1939\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1917 - val_loss: 1.1844\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1848 - val_loss: 1.1819\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1810 - val_loss: 1.1783\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1775 - val_loss: 1.1739\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1728 - val_loss: 1.1688\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1718 - val_loss: 1.1660\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1660 - val_loss: 1.1600\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1638 - val_loss: 1.1631\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1579 - val_loss: 1.1505\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1524 - val_loss: 1.1555\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1503 - val_loss: 1.1505\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1489 - val_loss: 1.1477\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1444 - val_loss: 1.1466\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1384 - val_loss: 1.1491\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1402 - val_loss: 1.1450\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1354 - val_loss: 1.1416\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1332 - val_loss: 1.1410\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1320 - val_loss: 1.1410\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1292 - val_loss: 1.1389\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1296 - val_loss: 1.1414\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1253 - val_loss: 1.1359\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1212 - val_loss: 1.1419\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1228 - val_loss: 1.1386\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1206 - val_loss: 1.1406\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1197 - val_loss: 1.1331\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.1186 - val_loss: 1.1332\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1150 - val_loss: 1.1409\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1137 - val_loss: 1.1295\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1136 - val_loss: 1.1350\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1118 - val_loss: 1.1334\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1081 - val_loss: 1.1293\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1066 - val_loss: 1.1271\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1037 - val_loss: 1.1291\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1012 - val_loss: 1.1285\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0996 - val_loss: 1.1302\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1000 - val_loss: 1.1274\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0962 - val_loss: 1.1302\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0936 - val_loss: 1.1359\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0922 - val_loss: 1.1334\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0915 - val_loss: 1.1235\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0899 - val_loss: 1.1242\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0837 - val_loss: 1.1425\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0844 - val_loss: 1.1310\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0834 - val_loss: 1.1242\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0811 - val_loss: 1.1277\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0762 - val_loss: 1.1388\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0754 - val_loss: 1.1322\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0749 - val_loss: 1.1337\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0746 - val_loss: 1.1362\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 4ms/step - loss: 1.0769 - val_loss: 1.1340\n",
            "Epoch 51: early stopping\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " encoder1 (Encoder1)            (None, 30)           20670       ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " encoder2 (Encoder2)            (None, 30)           38910       ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 60)           0           ['encoder1[1][0]',               \n",
            "                                                                  'encoder2[1][0]']               \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 64)           3904        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 64)          256         ['dense_3[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64)           0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 32)           2080        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 32)           0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 16)           528         ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 4)            68          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 2)            10          ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,426\n",
            "Trainable params: 6,718\n",
            "Non-trainable params: 59,708\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 2s 4ms/step - loss: 0.5028 - sparse_categorical_accuracy: 0.7570 - val_loss: 0.3445 - val_sparse_categorical_accuracy: 0.8804\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2928 - sparse_categorical_accuracy: 0.8860 - val_loss: 0.2834 - val_sparse_categorical_accuracy: 0.8906\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2486 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.2779 - val_sparse_categorical_accuracy: 0.8982\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2376 - sparse_categorical_accuracy: 0.9117 - val_loss: 0.2770 - val_sparse_categorical_accuracy: 0.9016\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2046 - sparse_categorical_accuracy: 0.9202 - val_loss: 0.2820 - val_sparse_categorical_accuracy: 0.9042\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2004 - sparse_categorical_accuracy: 0.9249 - val_loss: 0.2842 - val_sparse_categorical_accuracy: 0.8999\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.1925 - sparse_categorical_accuracy: 0.9266 - val_loss: 0.2900 - val_sparse_categorical_accuracy: 0.8982\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.1886 - sparse_categorical_accuracy: 0.9312 - val_loss: 0.2945 - val_sparse_categorical_accuracy: 0.8965\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.1828 - sparse_categorical_accuracy: 0.9348 - val_loss: 0.2882 - val_sparse_categorical_accuracy: 0.8957\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.1917 - sparse_categorical_accuracy: 0.9310 - val_loss: 0.2878 - val_sparse_categorical_accuracy: 0.8999\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.1787 - sparse_categorical_accuracy: 0.9368 - val_loss: 0.2897 - val_sparse_categorical_accuracy: 0.9008\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.1708 - sparse_categorical_accuracy: 0.9408 - val_loss: 0.2891 - val_sparse_categorical_accuracy: 0.8991\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.1698 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.2919 - val_sparse_categorical_accuracy: 0.9016\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.1721 - sparse_categorical_accuracy: 0.9376 - val_loss: 0.3042 - val_sparse_categorical_accuracy: 0.8991\n",
            "Epoch 14: early stopping\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2939 - sparse_categorical_accuracy: 0.9010\n",
            "150/150 [==============================] - 1s 2ms/step\n",
            "Test accuracy: 90.102%\n",
            "Postprocessing Test accuracy: 92.942%\n",
            "Test F1_score: 88.595%\n",
            "Postprocessing F1_score: 91.971%\n"
          ]
        }
      ],
      "source": [
        "y = np.concatenate((ytra_a,ytra_a), axis=0)\n",
        "yv = np.concatenate((yval_a,yval_a), axis=0)\n",
        "\n",
        "encoder1 = create_encoder1()\n",
        "encoder2 = create_encoder2()\n",
        "encoder_with_projection_head = add_projection_head1(encoder1, encoder2)\n",
        "encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),loss=SupervisedContrastiveLoss(temperature))\n",
        "encoder_with_projection_head.summary()\n",
        "                                                            #ytra_a                                  #yval_a\n",
        "history = encoder_with_projection_head.fit([xtra_a,xtra_ac], ytra_ac , validation_data =([xval_a,xval_ac],yval_ac), batch_size=32, epochs=100, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
        "\n",
        "\n",
        "learning_rate = 0.0005\n",
        "batch_size = 16\n",
        "hidden_units = 64\n",
        "projection_units = 128\n",
        "num_epochs = 200\n",
        "dropout_rate = 0.3\n",
        "num_classes = 2\n",
        "input_shape1 = (19,)\n",
        "input_shape2 = (38,)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
        "classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
        "classifier.summary()\n",
        "history = classifier.fit(x=[xtra_a,xtra_ac], y=ytra_a, validation_data =([xval_a,xval_ac],yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
        "\n",
        "accuracy = classifier.evaluate([xtest_a,xtest_ac], ytest)[1]\n",
        "\n",
        "##  Accuracy on post processed labels (each word should contain only one stressed syllable)\n",
        "pred_output= classifier.predict([xtest_a,xtest_ac])\n",
        "# pred_labels= pred_output.argmax(axis =1)\n",
        "pred1_labels = pred_output[:,1]\n",
        "post_labels = make_partitions(wtest, pred1_labels)\n",
        "post_accuracy = calculate_accuracy(post_labels, ytest)\n",
        "\n",
        "F1_score_WoPP = f1_score(ytest, pred_output.argmax(axis =1))\n",
        "F1_score_WPP = f1_score(ytest, post_labels)\n",
        "\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 3)}%\")\n",
        "print(f\"Postprocessing Test accuracy: {round(post_accuracy * 100, 3)}%\")\n",
        "print(f\"Test F1_score: {round(F1_score_WoPP * 100, 3)}%\")\n",
        "print(f\"Postprocessing F1_score: {round(F1_score_WPP * 100, 3)}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
