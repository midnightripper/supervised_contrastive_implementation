{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: matplotlib in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.2.2)\n",
            "Requirement already satisfied: tensorflow in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (2.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.39.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (23.5.9)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.55.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (4.23.1)\n",
            "Requirement already satisfied: setuptools in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (67.6.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 4)) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 4)) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow->-r requirements.txt (line 4)) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (2.18.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (2.28.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (2.3.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: urllib3<2.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (1.26.15)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 4)) (3.2.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm6CrX-XDs02",
        "outputId": "7bf1b09f-e6b8-4ece-fffa-dc037582af36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-23 10:56:45.550471: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-05-23 10:56:45.587450: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-05-23 10:56:45.588093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-23 10:56:46.609932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import backend as K\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Reshape\n",
        "temperature = 0.03\n",
        "learning_rate=0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "79UUK0g7D8BA"
      },
      "outputs": [],
      "source": [
        "def dataloader(path, featType):\n",
        "    \"\"\"\n",
        "    Load data from a MATLAB file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the MATLAB file.\n",
        "        featType (int): Type of features to load.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
        "    \"\"\"\n",
        "    data = scipy.io.loadmat(path)\n",
        "    print(data.keys())\n",
        "\n",
        "    AF = data['AF']\n",
        "    x1 = AF[:-2]\n",
        "    y = AF[-2]\n",
        "    w = AF[-1]\n",
        "\n",
        "    if featType == 1:\n",
        "        x = x1\n",
        "    else:\n",
        "        x2 = data['CF']\n",
        "        x = np.concatenate((x1, x2), axis=0)\n",
        "    return x.T, y.T, w.T, data['CF_info']\n",
        "\n",
        "def calculate_accuracy(arr1, arr2):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy between two arrays.\n",
        "\n",
        "    Args:\n",
        "        arr1 (array): First array.\n",
        "        arr2 (array): Second array.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy between the two arrays.\n",
        "    \"\"\"\n",
        "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
        "    return count / len(arr1)\n",
        "\n",
        "def normalization(feats):\n",
        "\n",
        "    \"\"\"\n",
        "    Normalize the input features using standard scaling.\n",
        "\n",
        "    Args:\n",
        "        feats (array): Input features.\n",
        "\n",
        "    Returns:\n",
        "        array: Normalized features.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(feats)\n",
        "    scaler = StandardScaler()\n",
        "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "    return x_new\n",
        "\n",
        "def make_partitions(arr_words, arr_labels):\n",
        "\n",
        "    \"\"\"\n",
        "    Create partitions based on word boundaries and labels.\n",
        "\n",
        "    Args:\n",
        "        arr_words (array): Array of words.\n",
        "        arr_labels (array): Array of labels.\n",
        "\n",
        "    Returns:\n",
        "        array: Partitions based on word boundaries and labels.\n",
        "    \"\"\"\n",
        "    v = []\n",
        "    temp = []\n",
        "\n",
        "    for i in range(len(arr_words) - 1):\n",
        "        word = arr_words[i]\n",
        "        next_word = arr_words[i + 1]\n",
        "        temp.append(arr_labels[i])\n",
        "\n",
        "        if word != next_word or i == len(arr_words) - 2:\n",
        "            if i == len(arr_words) - 2:\n",
        "                temp.append(arr_labels[i + 1])\n",
        "\n",
        "            numpy_temp = np.array(temp)\n",
        "            temp_max = np.amax(numpy_temp)\n",
        "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
        "            v = np.concatenate((v, numpy_temp), axis=None)\n",
        "            temp.clear()\n",
        "\n",
        "    v1 = [1 if i == 1 else 0 for i in v]\n",
        "    return v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1qgHWXIE7XE"
      },
      "source": [
        "#Flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5XhgKpZ_EwZR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "fatyp = 'TypicalFA_comb1'\n",
        "drivepath = 'finalData/'+ fatyp +'/';\n",
        "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
        "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
        "featType = 1; #Acoustic or Acoustic+context\n",
        "if featType == 1:\n",
        "  original_dim = 19\n",
        "else:\n",
        "  original_dim = 38"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
          ]
        }
      ],
      "source": [
        "train_path = filee; test_path = filee.replace('train','test')\n",
        "# print('test file:::::::',os.path.basename(test_path))\n",
        "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
        "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
        "\n",
        "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
        "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "xtest_a = normalization(xtest)\n",
        "xtest_ac = normalization(xtest1)\n",
        "xtrain = normalization(xtrain)\n",
        "xtrain1 = normalization(xtrain1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=temperature, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        print(feature_vectors.shape)\n",
        "        # labels = tf.keras.layers.Concatenate(axis=0)([labels, labels])\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = tf.divide(tf.matmul(feature_vectors_normalized, tf.transpose(feature_vectors_normalized)), self.temperature)\n",
        "        \n",
        "        # print(feature_vectors.shape)\n",
        "        # print(labels.shape)\n",
        "        # print('loss:::::::', tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
        "        return 0.35*(tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
        "        #find out more about why 0.35 is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_encoder1(latent_dim):\n",
        "    encoder_input = Input(shape=(19,1))\n",
        "    x1 = Conv1D(filters=64, kernel_size=2, activation='relu')(encoder_input)\n",
        "    x2 = MaxPooling1D(pool_size=1)(x1)\n",
        "    x3 = Dropout(0.5)(x2)\n",
        "    x4 = Conv1D(filters=32, kernel_size=2, activation='relu')(x3)\n",
        "    x5 = MaxPooling1D(pool_size=1)(x4)\n",
        "    x6 = Flatten()(x5)\n",
        "    output_layer = Dense(latent_dim, activation='gelu')(x6)\n",
        "    encoder1 = Model(encoder_input, output_layer)\n",
        "    return encoder1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_encoder2(latent_dim):\n",
        "    encoder_input = Input(shape=(38,1))\n",
        "    x1 = Conv1D(filters=64, kernel_size=2, activation='relu')(encoder_input)\n",
        "    x2 = MaxPooling1D(pool_size=1)(x1)\n",
        "    x3 = Dropout(0.5)(x2)\n",
        "    x4 = Conv1D(filters=32, kernel_size=2, activation='relu')(x3)\n",
        "    x5 = MaxPooling1D(pool_size=1)(x4)\n",
        "    x6 = Flatten()(x5)\n",
        "    output_layer = Dense(latent_dim, activation='gelu')(x6)\n",
        "    encoder2 = Model(encoder_input, output_layer)\n",
        "    return encoder2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# latent_dim=10\n",
        "# Encoder1=create_encoder1(latent_dim)\n",
        "# Encoder2=create_encoder2(latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_projection_head1(Encoder1, Encoder2):\n",
        "    inp1 = keras.Input(shape=input_shape1)\n",
        "    inp2 = keras.Input(shape=input_shape2)\n",
        "    hidden3a  = Encoder1(inp1)\n",
        "    hidden3b = Encoder2(inp2)\n",
        "    features = tf.keras.layers.Concatenate(axis=1)([hidden3a, hidden3b])\n",
        "    features = layers.Dense(16, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=[inp1, inp2], outputs=features, name=\"cifar-encoder_with_projection-head\"\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_classifier(encoder, trainable):\n",
        "\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "    inputs1 = keras.Input(shape=input_shape1)\n",
        "    inputs2 = keras.Input(shape=input_shape2)\n",
        "    features1 = encoder1(inputs1)\n",
        "    features2 = encoder2(inputs2)\n",
        "    features = tf.keras.layers.Concatenate(axis=1)([features1, features2])\n",
        "    # features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
        "    \n",
        "    features = layers.BatchNormalization()(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(32, activation=\"relu\")(features)\n",
        "    features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(16, activation=\"relu\")(features)\n",
        "    # features = layers.BatchNormalization()(features)\n",
        "    # features = layers.Dropout(0.2)(features)\n",
        "    features = layers.Dense(4, activation=\"relu\")(features)\n",
        "    # features = layers.BatchNormalization()(features)\n",
        "    # features = layers.Dropout(0.1)(features)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = xtrain.shape[0]\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1178, 19)\n",
            "(1178, 38)\n",
            "(1178,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_6 (Functional)           (None, 10)           9770        ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " model_7 (Functional)           (None, 10)           15850       ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 20)           0           ['model_6[0][0]',                \n",
            "                                                                  'model_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 16)           336         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "145/148 [============================>.] - ETA: 0s - loss: 1.2492(None, 16)\n",
            "148/148 [==============================] - 6s 14ms/step - loss: 1.2478 - val_loss: 1.2079\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.2063 - val_loss: 1.1896\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1970 - val_loss: 1.1783\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1909 - val_loss: 1.1720\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1881 - val_loss: 1.1691\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1848 - val_loss: 1.1655\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1814 - val_loss: 1.1613\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1800 - val_loss: 1.1594\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1783 - val_loss: 1.1567\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1747 - val_loss: 1.1587\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1716 - val_loss: 1.1507\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1693 - val_loss: 1.1503\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1666 - val_loss: 1.1480\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1655 - val_loss: 1.1457\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1620 - val_loss: 1.1456\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1620 - val_loss: 1.1439\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1587 - val_loss: 1.1401\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1587 - val_loss: 1.1460\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1552 - val_loss: 1.1448\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1566 - val_loss: 1.1416\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1549 - val_loss: 1.1402\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1516 - val_loss: 1.1425\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1506 - val_loss: 1.1392\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1505 - val_loss: 1.1389\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1472 - val_loss: 1.1418\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1457 - val_loss: 1.1419\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1475 - val_loss: 1.1354\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1455 - val_loss: 1.1379\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1412 - val_loss: 1.1365\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1419 - val_loss: 1.1378\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1385 - val_loss: 1.1375\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1392 - val_loss: 1.1311\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1370 - val_loss: 1.1348\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1314 - val_loss: 1.1352\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1318 - val_loss: 1.1325\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1281 - val_loss: 1.1292\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1244 - val_loss: 1.1305\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1265 - val_loss: 1.1298\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1242 - val_loss: 1.1308\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1222 - val_loss: 1.1297\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1194 - val_loss: 1.1271\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1216 - val_loss: 1.1230\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1192 - val_loss: 1.1281\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1151 - val_loss: 1.1317\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1132 - val_loss: 1.1291\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1129 - val_loss: 1.1294\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1121 - val_loss: 1.1226\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1121 - val_loss: 1.1212\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1110 - val_loss: 1.1277\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1087 - val_loss: 1.1285\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1090 - val_loss: 1.1261\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1052 - val_loss: 1.1250\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1060 - val_loss: 1.1193\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1020 - val_loss: 1.1255\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1003 - val_loss: 1.1289\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0950 - val_loss: 1.1222\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1008 - val_loss: 1.1396\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.0979 - val_loss: 1.1278\n",
            "Epoch 59/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0987 - val_loss: 1.1251\n",
            "Epoch 60/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0957 - val_loss: 1.1257\n",
            "Epoch 61/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1000 - val_loss: 1.1237\n",
            "Epoch 62/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0979 - val_loss: 1.1366\n",
            "Epoch 63/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0953 - val_loss: 1.1409\n",
            "Epoch 63: early stopping\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_6 (Functional)           (None, 10)           9770        ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " model_7 (Functional)           (None, 10)           15850       ['input_14[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 20)           0           ['model_6[1][0]',                \n",
            "                                                                  'model_7[1][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 64)           1344        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 64)          256         ['dense_9[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 64)           0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 32)           2080        ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 32)           0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 16)           528         ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 4)            68          ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 2)            10          ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 5s 6ms/step - loss: 0.6246 - sparse_categorical_accuracy: 0.7044 - val_loss: 0.4052 - val_sparse_categorical_accuracy: 0.8710\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3745 - sparse_categorical_accuracy: 0.8428 - val_loss: 0.2882 - val_sparse_categorical_accuracy: 0.8896\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3186 - sparse_categorical_accuracy: 0.8714 - val_loss: 0.2741 - val_sparse_categorical_accuracy: 0.8947\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2938 - sparse_categorical_accuracy: 0.8801 - val_loss: 0.2673 - val_sparse_categorical_accuracy: 0.8981\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2824 - sparse_categorical_accuracy: 0.8882 - val_loss: 0.2668 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2704 - sparse_categorical_accuracy: 0.8960 - val_loss: 0.2662 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2699 - sparse_categorical_accuracy: 0.9005 - val_loss: 0.2673 - val_sparse_categorical_accuracy: 0.8956\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2500 - sparse_categorical_accuracy: 0.9047 - val_loss: 0.2708 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2540 - sparse_categorical_accuracy: 0.9032 - val_loss: 0.2711 - val_sparse_categorical_accuracy: 0.8973\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2368 - sparse_categorical_accuracy: 0.9092 - val_loss: 0.2718 - val_sparse_categorical_accuracy: 0.8939\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2370 - sparse_categorical_accuracy: 0.9115 - val_loss: 0.2736 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2411 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.2735 - val_sparse_categorical_accuracy: 0.8981\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2277 - sparse_categorical_accuracy: 0.9183 - val_loss: 0.2743 - val_sparse_categorical_accuracy: 0.8981\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2317 - sparse_categorical_accuracy: 0.9151 - val_loss: 0.2695 - val_sparse_categorical_accuracy: 0.8956\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2321 - sparse_categorical_accuracy: 0.9124 - val_loss: 0.2736 - val_sparse_categorical_accuracy: 0.8964\n",
            "Epoch 16/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2192 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.2740 - val_sparse_categorical_accuracy: 0.8930\n",
            "Epoch 16: early stopping\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2984 - sparse_categorical_accuracy: 0.8839\n",
            "150/150 [==============================] - 1s 3ms/step\n",
            "Test accuracy: 88.39%\n",
            "Postprocessing Test accuracy: 91.982%\n",
            "Test F1_score: 86.93%\n",
            "Postprocessing F1_score: 90.879%\n",
            "(1178, 19)\n",
            "(1178, 38)\n",
            "(1178,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_17 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_9 (Functional)           (None, 10)           9770        ['input_17[0][0]']               \n",
            "                                                                                                  \n",
            " model_10 (Functional)          (None, 10)           15850       ['input_18[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 20)           0           ['model_9[0][0]',                \n",
            "                                                                  'model_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 16)           336         ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "148/148 [==============================] - ETA: 0s - loss: 1.3227(None, 16)\n",
            "148/148 [==============================] - 4s 9ms/step - loss: 1.3227 - val_loss: 1.2063\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.2088 - val_loss: 1.1964\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.2019 - val_loss: 1.1913\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1972 - val_loss: 1.1881\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1918 - val_loss: 1.1849\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1888 - val_loss: 1.1813\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1861 - val_loss: 1.1810\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1836 - val_loss: 1.1790\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1817 - val_loss: 1.1756\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1800 - val_loss: 1.1741\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1773 - val_loss: 1.1743\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1736 - val_loss: 1.1720\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1717 - val_loss: 1.1719\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1692 - val_loss: 1.1664\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1670 - val_loss: 1.1689\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1652 - val_loss: 1.1664\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1621 - val_loss: 1.1603\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1611 - val_loss: 1.1580\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1585 - val_loss: 1.1570\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1545 - val_loss: 1.1548\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1527 - val_loss: 1.1540\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1484 - val_loss: 1.1499\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1493 - val_loss: 1.1568\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1465 - val_loss: 1.1499\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1459 - val_loss: 1.1518\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1425 - val_loss: 1.1488\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1404 - val_loss: 1.1492\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1402 - val_loss: 1.1533\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1373 - val_loss: 1.1534\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1366 - val_loss: 1.1535\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1359 - val_loss: 1.1503\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1337 - val_loss: 1.1489\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1316 - val_loss: 1.1570\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1307 - val_loss: 1.1506\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1272 - val_loss: 1.1527\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1297 - val_loss: 1.1462\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1264 - val_loss: 1.1467\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1250 - val_loss: 1.1473\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1257 - val_loss: 1.1478\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1228 - val_loss: 1.1520\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1221 - val_loss: 1.1490\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1190 - val_loss: 1.1448\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1170 - val_loss: 1.1413\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1140 - val_loss: 1.1486\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1150 - val_loss: 1.1504\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1129 - val_loss: 1.1467\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1095 - val_loss: 1.1488\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1120 - val_loss: 1.1426\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1098 - val_loss: 1.1451\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1093 - val_loss: 1.1466\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1078 - val_loss: 1.1408\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1050 - val_loss: 1.1473\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1066 - val_loss: 1.1412\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1041 - val_loss: 1.1432\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1030 - val_loss: 1.1478\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1023 - val_loss: 1.1472\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1033 - val_loss: 1.1436\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1011 - val_loss: 1.1508\n",
            "Epoch 59/100\n",
            "148/148 [==============================] - 1s 10ms/step - loss: 1.1000 - val_loss: 1.1527\n",
            "Epoch 60/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0976 - val_loss: 1.1544\n",
            "Epoch 61/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0981 - val_loss: 1.1391\n",
            "Epoch 62/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0946 - val_loss: 1.1463\n",
            "Epoch 63/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.0977 - val_loss: 1.1432\n",
            "Epoch 64/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0995 - val_loss: 1.1370\n",
            "Epoch 65/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0941 - val_loss: 1.1443\n",
            "Epoch 66/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0933 - val_loss: 1.1425\n",
            "Epoch 67/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0940 - val_loss: 1.1437\n",
            "Epoch 68/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0939 - val_loss: 1.1417\n",
            "Epoch 69/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0925 - val_loss: 1.1410\n",
            "Epoch 70/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0907 - val_loss: 1.1440\n",
            "Epoch 71/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.0881 - val_loss: 1.1463\n",
            "Epoch 72/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.0893 - val_loss: 1.1477\n",
            "Epoch 73/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.0892 - val_loss: 1.1514\n",
            "Epoch 74/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.0876 - val_loss: 1.1488\n",
            "Epoch 74: early stopping\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_9 (Functional)           (None, 10)           9770        ['input_19[0][0]']               \n",
            "                                                                                                  \n",
            " model_10 (Functional)          (None, 10)           15850       ['input_20[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 20)           0           ['model_9[1][0]',                \n",
            "                                                                  'model_10[1][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 64)           1344        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 64)          256         ['dense_17[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 64)           0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 32)           2080        ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 32)           0           ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 16)           528         ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 4)            68          ['dense_19[0][0]']               \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 2)            10          ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 7s 10ms/step - loss: 0.4592 - sparse_categorical_accuracy: 0.7861 - val_loss: 0.3662 - val_sparse_categorical_accuracy: 0.8472\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.3001 - sparse_categorical_accuracy: 0.8799 - val_loss: 0.3257 - val_sparse_categorical_accuracy: 0.8735\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2716 - sparse_categorical_accuracy: 0.8948 - val_loss: 0.3247 - val_sparse_categorical_accuracy: 0.8727\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2545 - sparse_categorical_accuracy: 0.9056 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8752\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2476 - sparse_categorical_accuracy: 0.9081 - val_loss: 0.3177 - val_sparse_categorical_accuracy: 0.8786\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2321 - sparse_categorical_accuracy: 0.9179 - val_loss: 0.3292 - val_sparse_categorical_accuracy: 0.8744\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2262 - sparse_categorical_accuracy: 0.9177 - val_loss: 0.3305 - val_sparse_categorical_accuracy: 0.8744\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2157 - sparse_categorical_accuracy: 0.9236 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.8727\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2226 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3242 - val_sparse_categorical_accuracy: 0.8778\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.2166 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.3254 - val_sparse_categorical_accuracy: 0.8752\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2029 - sparse_categorical_accuracy: 0.9259 - val_loss: 0.3339 - val_sparse_categorical_accuracy: 0.8744\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.2067 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.3311 - val_sparse_categorical_accuracy: 0.8752\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.2104 - sparse_categorical_accuracy: 0.9209 - val_loss: 0.3299 - val_sparse_categorical_accuracy: 0.8752\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.2067 - sparse_categorical_accuracy: 0.9272 - val_loss: 0.3255 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2167 - sparse_categorical_accuracy: 0.9219 - val_loss: 0.3264 - val_sparse_categorical_accuracy: 0.8735\n",
            "Epoch 15: early stopping\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2841 - sparse_categorical_accuracy: 0.8964\n",
            "150/150 [==============================] - 1s 3ms/step\n",
            "Test accuracy: 89.643%\n",
            "Postprocessing Test accuracy: 92.859%\n",
            "Test F1_score: 88.302%\n",
            "Postprocessing F1_score: 91.876%\n",
            "(1178, 19)\n",
            "(1178, 38)\n",
            "(1178,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_23 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_12 (Functional)          (None, 10)           9770        ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " model_13 (Functional)          (None, 10)           15850       ['input_24[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 20)           0           ['model_12[0][0]',               \n",
            "                                                                  'model_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 16)           336         ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "143/148 [===========================>..] - ETA: 0s - loss: 1.3112(None, 16)\n",
            "148/148 [==============================] - 5s 12ms/step - loss: 1.3076 - val_loss: 1.2090\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.2114 - val_loss: 1.2023\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.2035 - val_loss: 1.1969\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1973 - val_loss: 1.1944\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1934 - val_loss: 1.1910\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1901 - val_loss: 1.1884\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1872 - val_loss: 1.1844\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1859 - val_loss: 1.1842\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1814 - val_loss: 1.1802\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1791 - val_loss: 1.1782\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1774 - val_loss: 1.1758\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1740 - val_loss: 1.1734\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1731 - val_loss: 1.1734\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1715 - val_loss: 1.1700\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1692 - val_loss: 1.1723\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1669 - val_loss: 1.1676\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1649 - val_loss: 1.1657\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1643 - val_loss: 1.1656\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1627 - val_loss: 1.1601\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1594 - val_loss: 1.1617\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1583 - val_loss: 1.1578\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1526 - val_loss: 1.1600\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1534 - val_loss: 1.1599\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1486 - val_loss: 1.1553\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1489 - val_loss: 1.1511\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1459 - val_loss: 1.1504\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 10ms/step - loss: 1.1433 - val_loss: 1.1502\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1423 - val_loss: 1.1511\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1422 - val_loss: 1.1471\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1368 - val_loss: 1.1499\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1382 - val_loss: 1.1422\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1359 - val_loss: 1.1433\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1357 - val_loss: 1.1436\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1326 - val_loss: 1.1402\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1290 - val_loss: 1.1449\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1291 - val_loss: 1.1410\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1258 - val_loss: 1.1375\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1274 - val_loss: 1.1372\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1250 - val_loss: 1.1362\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1229 - val_loss: 1.1398\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1245 - val_loss: 1.1418\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1223 - val_loss: 1.1427\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1205 - val_loss: 1.1363\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1192 - val_loss: 1.1416\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1188 - val_loss: 1.1331\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1205 - val_loss: 1.1383\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1145 - val_loss: 1.1357\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1157 - val_loss: 1.1421\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1149 - val_loss: 1.1418\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1143 - val_loss: 1.1356\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1118 - val_loss: 1.1340\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1072 - val_loss: 1.1347\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1111 - val_loss: 1.1354\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1059 - val_loss: 1.1339\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1072 - val_loss: 1.1361\n",
            "Epoch 55: early stopping\n",
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_25 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_26 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_12 (Functional)          (None, 10)           9770        ['input_25[0][0]']               \n",
            "                                                                                                  \n",
            " model_13 (Functional)          (None, 10)           15850       ['input_26[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 20)           0           ['model_12[1][0]',               \n",
            "                                                                  'model_13[1][0]']               \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 64)           1344        ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 64)          256         ['dense_25[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 64)           0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 32)           2080        ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 32)           0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 16)           528         ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 4)            68          ['dense_27[0][0]']               \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 2)            10          ['dense_28[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 6s 7ms/step - loss: 0.5010 - sparse_categorical_accuracy: 0.7590 - val_loss: 0.3606 - val_sparse_categorical_accuracy: 0.8693\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.8774 - val_loss: 0.3020 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2839 - sparse_categorical_accuracy: 0.8960 - val_loss: 0.2999 - val_sparse_categorical_accuracy: 0.8795\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2772 - sparse_categorical_accuracy: 0.8888 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2648 - sparse_categorical_accuracy: 0.8967 - val_loss: 0.3048 - val_sparse_categorical_accuracy: 0.8710\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2610 - sparse_categorical_accuracy: 0.9020 - val_loss: 0.3022 - val_sparse_categorical_accuracy: 0.8752\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2608 - sparse_categorical_accuracy: 0.8990 - val_loss: 0.3049 - val_sparse_categorical_accuracy: 0.8778\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2622 - sparse_categorical_accuracy: 0.8984 - val_loss: 0.2991 - val_sparse_categorical_accuracy: 0.8769\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2597 - sparse_categorical_accuracy: 0.9022 - val_loss: 0.2988 - val_sparse_categorical_accuracy: 0.8778\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2573 - sparse_categorical_accuracy: 0.9026 - val_loss: 0.3056 - val_sparse_categorical_accuracy: 0.8752\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2543 - sparse_categorical_accuracy: 0.8982 - val_loss: 0.3009 - val_sparse_categorical_accuracy: 0.8786\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2508 - sparse_categorical_accuracy: 0.9024 - val_loss: 0.3049 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2543 - sparse_categorical_accuracy: 0.8999 - val_loss: 0.3047 - val_sparse_categorical_accuracy: 0.8727\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2633 - sparse_categorical_accuracy: 0.8990 - val_loss: 0.2991 - val_sparse_categorical_accuracy: 0.8795\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2426 - sparse_categorical_accuracy: 0.9047 - val_loss: 0.3042 - val_sparse_categorical_accuracy: 0.8786\n",
            "Epoch 16/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2493 - sparse_categorical_accuracy: 0.9037 - val_loss: 0.3034 - val_sparse_categorical_accuracy: 0.8769\n",
            "Epoch 17/200\n",
            "295/295 [==============================] - 2s 5ms/step - loss: 0.2387 - sparse_categorical_accuracy: 0.9143 - val_loss: 0.3132 - val_sparse_categorical_accuracy: 0.8735\n",
            "Epoch 18/200\n",
            "295/295 [==============================] - 1s 5ms/step - loss: 0.2482 - sparse_categorical_accuracy: 0.9052 - val_loss: 0.3077 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 19/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2401 - sparse_categorical_accuracy: 0.9045 - val_loss: 0.3125 - val_sparse_categorical_accuracy: 0.8795\n",
            "Epoch 19: early stopping\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2868 - sparse_categorical_accuracy: 0.8900\n",
            "150/150 [==============================] - 1s 3ms/step\n",
            "Test accuracy: 88.996%\n",
            "Postprocessing Test accuracy: 92.525%\n",
            "Test F1_score: 87.626%\n",
            "Postprocessing F1_score: 91.496%\n",
            "(1178, 19)\n",
            "(1178, 38)\n",
            "(1178,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_15 (Functional)          (None, 10)           9770        ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " model_16 (Functional)          (None, 10)           15850       ['input_30[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 20)           0           ['model_15[0][0]',               \n",
            "                                                                  'model_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 16)           336         ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "148/148 [==============================] - ETA: 0s - loss: 1.3513(None, 16)\n",
            "148/148 [==============================] - 5s 13ms/step - loss: 1.3513 - val_loss: 1.2111\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.2156 - val_loss: 1.2104\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.2122 - val_loss: 1.2021\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.2038 - val_loss: 1.1900\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1969 - val_loss: 1.1863\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1920 - val_loss: 1.1830\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1887 - val_loss: 1.1822\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 9ms/step - loss: 1.1872 - val_loss: 1.1829\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1854 - val_loss: 1.1788\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1838 - val_loss: 1.1773\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1820 - val_loss: 1.1775\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1785 - val_loss: 1.1740\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1777 - val_loss: 1.1744\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1751 - val_loss: 1.1720\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1753 - val_loss: 1.1699\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1699 - val_loss: 1.1679\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1689 - val_loss: 1.1652\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1702 - val_loss: 1.1642\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1658 - val_loss: 1.1626\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1642 - val_loss: 1.1613\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1603 - val_loss: 1.1614\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1602 - val_loss: 1.1572\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1573 - val_loss: 1.1590\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1559 - val_loss: 1.1552\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1550 - val_loss: 1.1519\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1511 - val_loss: 1.1518\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1502 - val_loss: 1.1487\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1495 - val_loss: 1.1514\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1444 - val_loss: 1.1513\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1446 - val_loss: 1.1485\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1437 - val_loss: 1.1461\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1441 - val_loss: 1.1446\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1402 - val_loss: 1.1426\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1395 - val_loss: 1.1434\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1393 - val_loss: 1.1421\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1362 - val_loss: 1.1453\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1351 - val_loss: 1.1408\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1331 - val_loss: 1.1396\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1327 - val_loss: 1.1401\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1331 - val_loss: 1.1440\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1335 - val_loss: 1.1375\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1317 - val_loss: 1.1379\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1278 - val_loss: 1.1387\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1290 - val_loss: 1.1373\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1258 - val_loss: 1.1377\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1253 - val_loss: 1.1396\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1275 - val_loss: 1.1352\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1227 - val_loss: 1.1359\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1209 - val_loss: 1.1364\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1243 - val_loss: 1.1354\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1221 - val_loss: 1.1350\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1202 - val_loss: 1.1368\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1185 - val_loss: 1.1315\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1189 - val_loss: 1.1298\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1175 - val_loss: 1.1330\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1159 - val_loss: 1.1328\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1145 - val_loss: 1.1320\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 8ms/step - loss: 1.1122 - val_loss: 1.1283\n",
            "Epoch 59/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1146 - val_loss: 1.1323\n",
            "Epoch 60/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1124 - val_loss: 1.1304\n",
            "Epoch 61/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1118 - val_loss: 1.1288\n",
            "Epoch 62/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1093 - val_loss: 1.1311\n",
            "Epoch 63/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1093 - val_loss: 1.1304\n",
            "Epoch 64/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1121 - val_loss: 1.1294\n",
            "Epoch 65/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1082 - val_loss: 1.1309\n",
            "Epoch 66/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1095 - val_loss: 1.1266\n",
            "Epoch 67/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1076 - val_loss: 1.1278\n",
            "Epoch 68/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1075 - val_loss: 1.1278\n",
            "Epoch 69/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1088 - val_loss: 1.1278\n",
            "Epoch 70/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1047 - val_loss: 1.1351\n",
            "Epoch 71/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1010 - val_loss: 1.1330\n",
            "Epoch 72/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1017 - val_loss: 1.1309\n",
            "Epoch 73/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1041 - val_loss: 1.1346\n",
            "Epoch 74/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1019 - val_loss: 1.1291\n",
            "Epoch 75/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1026 - val_loss: 1.1309\n",
            "Epoch 76/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1005 - val_loss: 1.1271\n",
            "Epoch 76: early stopping\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_31 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_32 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_15 (Functional)          (None, 10)           9770        ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " model_16 (Functional)          (None, 10)           15850       ['input_32[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 20)           0           ['model_15[1][0]',               \n",
            "                                                                  'model_16[1][0]']               \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (None, 64)           1344        ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 64)          256         ['dense_33[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 64)           0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 32)           2080        ['dropout_20[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 32)           0           ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 16)           528         ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 4)            68          ['dense_35[0][0]']               \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (None, 2)            10          ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 5s 6ms/step - loss: 0.6288 - sparse_categorical_accuracy: 0.7136 - val_loss: 0.5266 - val_sparse_categorical_accuracy: 0.8701\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.5088 - sparse_categorical_accuracy: 0.8472 - val_loss: 0.4499 - val_sparse_categorical_accuracy: 0.8803\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.4547 - sparse_categorical_accuracy: 0.8676 - val_loss: 0.4071 - val_sparse_categorical_accuracy: 0.8896\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3967 - sparse_categorical_accuracy: 0.8852 - val_loss: 0.3810 - val_sparse_categorical_accuracy: 0.8896\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3723 - sparse_categorical_accuracy: 0.8869 - val_loss: 0.3568 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3618 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.3401 - val_sparse_categorical_accuracy: 0.8939\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3403 - sparse_categorical_accuracy: 0.8926 - val_loss: 0.3321 - val_sparse_categorical_accuracy: 0.8871\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3211 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.3268 - val_sparse_categorical_accuracy: 0.8846\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2979 - sparse_categorical_accuracy: 0.9058 - val_loss: 0.3295 - val_sparse_categorical_accuracy: 0.8837\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2954 - sparse_categorical_accuracy: 0.9060 - val_loss: 0.3133 - val_sparse_categorical_accuracy: 0.8854\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3001 - sparse_categorical_accuracy: 0.9005 - val_loss: 0.3061 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2893 - sparse_categorical_accuracy: 0.9035 - val_loss: 0.2990 - val_sparse_categorical_accuracy: 0.8862\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2870 - sparse_categorical_accuracy: 0.9005 - val_loss: 0.2954 - val_sparse_categorical_accuracy: 0.8871\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2766 - sparse_categorical_accuracy: 0.9079 - val_loss: 0.2950 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2807 - sparse_categorical_accuracy: 0.9047 - val_loss: 0.2892 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 16/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2699 - sparse_categorical_accuracy: 0.9092 - val_loss: 0.2882 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 17/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2780 - sparse_categorical_accuracy: 0.8994 - val_loss: 0.2880 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 18/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2602 - sparse_categorical_accuracy: 0.9102 - val_loss: 0.2909 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 19/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2601 - sparse_categorical_accuracy: 0.9094 - val_loss: 0.2862 - val_sparse_categorical_accuracy: 0.8871\n",
            "Epoch 20/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2567 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.2880 - val_sparse_categorical_accuracy: 0.8837\n",
            "Epoch 21/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2489 - sparse_categorical_accuracy: 0.9139 - val_loss: 0.2945 - val_sparse_categorical_accuracy: 0.8829\n",
            "Epoch 22/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2467 - sparse_categorical_accuracy: 0.9160 - val_loss: 0.2864 - val_sparse_categorical_accuracy: 0.8829\n",
            "Epoch 23/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2422 - sparse_categorical_accuracy: 0.9134 - val_loss: 0.2881 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 24/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2461 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.2843 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 25/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2435 - sparse_categorical_accuracy: 0.9175 - val_loss: 0.2873 - val_sparse_categorical_accuracy: 0.8829\n",
            "Epoch 26/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2493 - sparse_categorical_accuracy: 0.9086 - val_loss: 0.2811 - val_sparse_categorical_accuracy: 0.8846\n",
            "Epoch 27/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2499 - sparse_categorical_accuracy: 0.9115 - val_loss: 0.2805 - val_sparse_categorical_accuracy: 0.8829\n",
            "Epoch 28/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2481 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.2914 - val_sparse_categorical_accuracy: 0.8854\n",
            "Epoch 29/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2558 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.2851 - val_sparse_categorical_accuracy: 0.8854\n",
            "Epoch 30/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2365 - sparse_categorical_accuracy: 0.9153 - val_loss: 0.2816 - val_sparse_categorical_accuracy: 0.8871\n",
            "Epoch 31/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2401 - sparse_categorical_accuracy: 0.9128 - val_loss: 0.2824 - val_sparse_categorical_accuracy: 0.8837\n",
            "Epoch 32/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2335 - sparse_categorical_accuracy: 0.9128 - val_loss: 0.2786 - val_sparse_categorical_accuracy: 0.8888\n",
            "Epoch 33/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2401 - sparse_categorical_accuracy: 0.9098 - val_loss: 0.2792 - val_sparse_categorical_accuracy: 0.8871\n",
            "Epoch 34/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2417 - sparse_categorical_accuracy: 0.9130 - val_loss: 0.2828 - val_sparse_categorical_accuracy: 0.8854\n",
            "Epoch 35/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2439 - sparse_categorical_accuracy: 0.9160 - val_loss: 0.2827 - val_sparse_categorical_accuracy: 0.8829\n",
            "Epoch 36/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2293 - sparse_categorical_accuracy: 0.9189 - val_loss: 0.2805 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 37/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2392 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.2761 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 38/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2423 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.2832 - val_sparse_categorical_accuracy: 0.8871\n",
            "Epoch 39/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2449 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.2813 - val_sparse_categorical_accuracy: 0.8862\n",
            "Epoch 40/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2338 - sparse_categorical_accuracy: 0.9134 - val_loss: 0.2787 - val_sparse_categorical_accuracy: 0.8854\n",
            "Epoch 41/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2431 - sparse_categorical_accuracy: 0.9107 - val_loss: 0.2807 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 42/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2304 - sparse_categorical_accuracy: 0.9194 - val_loss: 0.2869 - val_sparse_categorical_accuracy: 0.8803\n",
            "Epoch 43/200\n",
            "295/295 [==============================] - 1s 3ms/step - loss: 0.2280 - sparse_categorical_accuracy: 0.9149 - val_loss: 0.2790 - val_sparse_categorical_accuracy: 0.8862\n",
            "Epoch 44/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2248 - sparse_categorical_accuracy: 0.9170 - val_loss: 0.2832 - val_sparse_categorical_accuracy: 0.8879\n",
            "Epoch 45/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2253 - sparse_categorical_accuracy: 0.9179 - val_loss: 0.2824 - val_sparse_categorical_accuracy: 0.8820\n",
            "Epoch 46/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2313 - sparse_categorical_accuracy: 0.9175 - val_loss: 0.2801 - val_sparse_categorical_accuracy: 0.8862\n",
            "Epoch 47/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2369 - sparse_categorical_accuracy: 0.9160 - val_loss: 0.2779 - val_sparse_categorical_accuracy: 0.8829\n",
            "Epoch 47: early stopping\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3006 - sparse_categorical_accuracy: 0.8849\n",
            "150/150 [==============================] - 1s 3ms/step\n",
            "Test accuracy: 88.494%\n",
            "Postprocessing Test accuracy: 91.46%\n",
            "Test F1_score: 87.069%\n",
            "Postprocessing F1_score: 90.324%\n",
            "(1179, 19)\n",
            "(1179, 38)\n",
            "(1179,)\n",
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_35 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_18 (Functional)          (None, 10)           9770        ['input_35[0][0]']               \n",
            "                                                                                                  \n",
            " model_19 (Functional)          (None, 10)           15850       ['input_36[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 20)           0           ['model_18[0][0]',               \n",
            "                                                                  'model_19[0][0]']               \n",
            "                                                                                                  \n",
            " dense_40 (Dense)               (None, 16)           336         ['concatenate_8[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,956\n",
            "Trainable params: 25,956\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "(None, 16)\n",
            "(None, 16)\n",
            "146/148 [============================>.] - ETA: 0s - loss: 1.2886(None, 16)\n",
            "148/148 [==============================] - 4s 11ms/step - loss: 1.2872 - val_loss: 1.2008\n",
            "Epoch 2/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.2045 - val_loss: 1.1916\n",
            "Epoch 3/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1964 - val_loss: 1.1856\n",
            "Epoch 4/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1921 - val_loss: 1.1826\n",
            "Epoch 5/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1881 - val_loss: 1.1796\n",
            "Epoch 6/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1830 - val_loss: 1.1774\n",
            "Epoch 7/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1817 - val_loss: 1.1757\n",
            "Epoch 8/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1786 - val_loss: 1.1735\n",
            "Epoch 9/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1780 - val_loss: 1.1709\n",
            "Epoch 10/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1744 - val_loss: 1.1698\n",
            "Epoch 11/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1721 - val_loss: 1.1673\n",
            "Epoch 12/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1700 - val_loss: 1.1670\n",
            "Epoch 13/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1673 - val_loss: 1.1605\n",
            "Epoch 14/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1646 - val_loss: 1.1599\n",
            "Epoch 15/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1621 - val_loss: 1.1581\n",
            "Epoch 16/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1597 - val_loss: 1.1556\n",
            "Epoch 17/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1571 - val_loss: 1.1540\n",
            "Epoch 18/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1559 - val_loss: 1.1512\n",
            "Epoch 19/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1508 - val_loss: 1.1537\n",
            "Epoch 20/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1522 - val_loss: 1.1507\n",
            "Epoch 21/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1472 - val_loss: 1.1468\n",
            "Epoch 22/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1449 - val_loss: 1.1474\n",
            "Epoch 23/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1442 - val_loss: 1.1475\n",
            "Epoch 24/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1417 - val_loss: 1.1530\n",
            "Epoch 25/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1395 - val_loss: 1.1506\n",
            "Epoch 26/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1409 - val_loss: 1.1481\n",
            "Epoch 27/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1349 - val_loss: 1.1498\n",
            "Epoch 28/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1328 - val_loss: 1.1472\n",
            "Epoch 29/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1334 - val_loss: 1.1528\n",
            "Epoch 30/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1330 - val_loss: 1.1472\n",
            "Epoch 31/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1299 - val_loss: 1.1437\n",
            "Epoch 32/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1309 - val_loss: 1.1456\n",
            "Epoch 33/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1293 - val_loss: 1.1440\n",
            "Epoch 34/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1273 - val_loss: 1.1453\n",
            "Epoch 35/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1245 - val_loss: 1.1462\n",
            "Epoch 36/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1237 - val_loss: 1.1423\n",
            "Epoch 37/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1215 - val_loss: 1.1512\n",
            "Epoch 38/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1207 - val_loss: 1.1429\n",
            "Epoch 39/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1216 - val_loss: 1.1410\n",
            "Epoch 40/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1161 - val_loss: 1.1441\n",
            "Epoch 41/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1185 - val_loss: 1.1436\n",
            "Epoch 42/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1137 - val_loss: 1.1484\n",
            "Epoch 43/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1140 - val_loss: 1.1442\n",
            "Epoch 44/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1127 - val_loss: 1.1413\n",
            "Epoch 45/100\n",
            "148/148 [==============================] - 1s 7ms/step - loss: 1.1115 - val_loss: 1.1431\n",
            "Epoch 46/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1109 - val_loss: 1.1399\n",
            "Epoch 47/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1122 - val_loss: 1.1434\n",
            "Epoch 48/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1096 - val_loss: 1.1443\n",
            "Epoch 49/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1056 - val_loss: 1.1447\n",
            "Epoch 50/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1075 - val_loss: 1.1414\n",
            "Epoch 51/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.1076 - val_loss: 1.1521\n",
            "Epoch 52/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1042 - val_loss: 1.1452\n",
            "Epoch 53/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1063 - val_loss: 1.1433\n",
            "Epoch 54/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1026 - val_loss: 1.1385\n",
            "Epoch 55/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1023 - val_loss: 1.1482\n",
            "Epoch 56/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.1003 - val_loss: 1.1457\n",
            "Epoch 57/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0996 - val_loss: 1.1423\n",
            "Epoch 58/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0994 - val_loss: 1.1504\n",
            "Epoch 59/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0979 - val_loss: 1.1435\n",
            "Epoch 60/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0964 - val_loss: 1.1578\n",
            "Epoch 61/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0997 - val_loss: 1.1524\n",
            "Epoch 62/100\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 1.0953 - val_loss: 1.1509\n",
            "Epoch 63/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0980 - val_loss: 1.1554\n",
            "Epoch 64/100\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 1.0944 - val_loss: 1.1445\n",
            "Epoch 64: early stopping\n",
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_37 (InputLayer)          [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " input_38 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " model_18 (Functional)          (None, 10)           9770        ['input_37[0][0]']               \n",
            "                                                                                                  \n",
            " model_19 (Functional)          (None, 10)           15850       ['input_38[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 20)           0           ['model_18[1][0]',               \n",
            "                                                                  'model_19[1][0]']               \n",
            "                                                                                                  \n",
            " dense_41 (Dense)               (None, 64)           1344        ['concatenate_9[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 64)          256         ['dense_41[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 64)           0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_42 (Dense)               (None, 32)           2080        ['dropout_24[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 32)           0           ['dense_42[0][0]']               \n",
            "                                                                                                  \n",
            " dense_43 (Dense)               (None, 16)           528         ['dropout_25[0][0]']             \n",
            "                                                                                                  \n",
            " dense_44 (Dense)               (None, 4)            68          ['dense_43[0][0]']               \n",
            "                                                                                                  \n",
            " dense_45 (Dense)               (None, 2)            10          ['dense_44[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,906\n",
            "Trainable params: 4,158\n",
            "Non-trainable params: 25,748\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "295/295 [==============================] - 5s 6ms/step - loss: 0.6373 - sparse_categorical_accuracy: 0.6721 - val_loss: 0.5472 - val_sparse_categorical_accuracy: 0.8397\n",
            "Epoch 2/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.4833 - sparse_categorical_accuracy: 0.8580 - val_loss: 0.4708 - val_sparse_categorical_accuracy: 0.8626\n",
            "Epoch 3/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.4241 - sparse_categorical_accuracy: 0.8837 - val_loss: 0.4597 - val_sparse_categorical_accuracy: 0.8609\n",
            "Epoch 4/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.8990 - val_loss: 0.4293 - val_sparse_categorical_accuracy: 0.8651\n",
            "Epoch 5/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3567 - sparse_categorical_accuracy: 0.9028 - val_loss: 0.4040 - val_sparse_categorical_accuracy: 0.8643\n",
            "Epoch 6/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3345 - sparse_categorical_accuracy: 0.9026 - val_loss: 0.3891 - val_sparse_categorical_accuracy: 0.8660\n",
            "Epoch 7/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3269 - sparse_categorical_accuracy: 0.9000 - val_loss: 0.3741 - val_sparse_categorical_accuracy: 0.8702\n",
            "Epoch 8/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.3055 - sparse_categorical_accuracy: 0.9079 - val_loss: 0.3667 - val_sparse_categorical_accuracy: 0.8694\n",
            "Epoch 9/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2958 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3544 - val_sparse_categorical_accuracy: 0.8702\n",
            "Epoch 10/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2879 - sparse_categorical_accuracy: 0.9109 - val_loss: 0.3602 - val_sparse_categorical_accuracy: 0.8702\n",
            "Epoch 11/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2845 - sparse_categorical_accuracy: 0.9051 - val_loss: 0.3488 - val_sparse_categorical_accuracy: 0.8711\n",
            "Epoch 12/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2737 - sparse_categorical_accuracy: 0.9107 - val_loss: 0.3467 - val_sparse_categorical_accuracy: 0.8753\n",
            "Epoch 13/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2659 - sparse_categorical_accuracy: 0.9134 - val_loss: 0.3435 - val_sparse_categorical_accuracy: 0.8762\n",
            "Epoch 14/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2603 - sparse_categorical_accuracy: 0.9145 - val_loss: 0.3458 - val_sparse_categorical_accuracy: 0.8779\n",
            "Epoch 15/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2581 - sparse_categorical_accuracy: 0.9153 - val_loss: 0.3395 - val_sparse_categorical_accuracy: 0.8779\n",
            "Epoch 16/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2541 - sparse_categorical_accuracy: 0.9155 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.8804\n",
            "Epoch 17/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2532 - sparse_categorical_accuracy: 0.9130 - val_loss: 0.3377 - val_sparse_categorical_accuracy: 0.8753\n",
            "Epoch 18/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2478 - sparse_categorical_accuracy: 0.9191 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8745\n",
            "Epoch 19/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2595 - sparse_categorical_accuracy: 0.9113 - val_loss: 0.3261 - val_sparse_categorical_accuracy: 0.8770\n",
            "Epoch 20/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2435 - sparse_categorical_accuracy: 0.9179 - val_loss: 0.3324 - val_sparse_categorical_accuracy: 0.8728\n",
            "Epoch 21/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2453 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.3377 - val_sparse_categorical_accuracy: 0.8779\n",
            "Epoch 22/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2522 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.3348 - val_sparse_categorical_accuracy: 0.8796\n",
            "Epoch 23/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2339 - sparse_categorical_accuracy: 0.9174 - val_loss: 0.3337 - val_sparse_categorical_accuracy: 0.8745\n",
            "Epoch 24/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2402 - sparse_categorical_accuracy: 0.9160 - val_loss: 0.3330 - val_sparse_categorical_accuracy: 0.8745\n",
            "Epoch 25/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2323 - sparse_categorical_accuracy: 0.9196 - val_loss: 0.3349 - val_sparse_categorical_accuracy: 0.8762\n",
            "Epoch 26/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2327 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.3239 - val_sparse_categorical_accuracy: 0.8719\n",
            "Epoch 27/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2388 - sparse_categorical_accuracy: 0.9149 - val_loss: 0.3315 - val_sparse_categorical_accuracy: 0.8779\n",
            "Epoch 28/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2294 - sparse_categorical_accuracy: 0.9194 - val_loss: 0.3276 - val_sparse_categorical_accuracy: 0.8728\n",
            "Epoch 29/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2258 - sparse_categorical_accuracy: 0.9191 - val_loss: 0.3381 - val_sparse_categorical_accuracy: 0.8770\n",
            "Epoch 30/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2329 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3285 - val_sparse_categorical_accuracy: 0.8779\n",
            "Epoch 31/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2234 - sparse_categorical_accuracy: 0.9191 - val_loss: 0.3305 - val_sparse_categorical_accuracy: 0.8796\n",
            "Epoch 32/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2202 - sparse_categorical_accuracy: 0.9281 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.8753\n",
            "Epoch 33/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2304 - sparse_categorical_accuracy: 0.9223 - val_loss: 0.3383 - val_sparse_categorical_accuracy: 0.8719\n",
            "Epoch 34/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2221 - sparse_categorical_accuracy: 0.9202 - val_loss: 0.3375 - val_sparse_categorical_accuracy: 0.8736\n",
            "Epoch 35/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2309 - sparse_categorical_accuracy: 0.9202 - val_loss: 0.3285 - val_sparse_categorical_accuracy: 0.8719\n",
            "Epoch 36/200\n",
            "295/295 [==============================] - 1s 4ms/step - loss: 0.2266 - sparse_categorical_accuracy: 0.9223 - val_loss: 0.3254 - val_sparse_categorical_accuracy: 0.8736\n",
            "Epoch 36: early stopping\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2922 - sparse_categorical_accuracy: 0.8860\n",
            "150/150 [==============================] - 1s 3ms/step\n",
            "Test accuracy: 88.599%\n",
            "Postprocessing Test accuracy: 92.796%\n",
            "Test F1_score: 87.019%\n",
            "Postprocessing F1_score: 91.846%\n"
          ]
        }
      ],
      "source": [
        "for j in range(0,5): # folds \n",
        "\n",
        "  xval_a = xtrain[(train_size*j)//5:(train_size*(j+1))//5];\n",
        "  print(xval_a.shape)\n",
        "  yval_a = ytrain[(train_size*j)//5:(train_size*(j+1))//5]\n",
        "  xtra_a = np.concatenate((xtrain[:(train_size*j)//5], xtrain[((train_size*(j+1))//5):]), axis=0) \n",
        "  ytra_a = np.concatenate((ytrain[:(train_size*j)//5], ytrain[((train_size*(j+1))//5):]), axis=0)\n",
        "  # print('Fold  '+str(j+1)+'  xtrain shape:::::::', xtra_a.shape)\n",
        "\n",
        "  xval_ac = xtrain1[(train_size*j)//5:(train_size*(j+1))//5];\n",
        "  print(xval_ac.shape)\n",
        "  yval_ac = ytrain1[(train_size*j)//5:(train_size*(j+1))//5]\n",
        "  xtra_ac = np.concatenate((xtrain1[:(train_size*j)//5], xtrain1[((train_size*(j+1))//5):]), axis=0)\n",
        "  ytra_ac = np.concatenate((ytrain1[:(train_size*j)//5], ytrain1[((train_size*(j+1))//5):]), axis=0)\n",
        "  y = np.concatenate((ytra_a,ytra_a), axis=0)\n",
        "  yv = np.concatenate((yval_a,yval_a), axis=0)\n",
        "  print(yval_ac.shape)\n",
        "  #check what's happening here is it intended\n",
        "  encoder1 = create_encoder1(latent_dim=10)\n",
        "  encoder2 = create_encoder2(latent_dim=10)\n",
        "  encoder_with_projection_head = add_projection_head1(encoder1,encoder2)\n",
        "  encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),loss=SupervisedContrastiveLoss(temperature))\n",
        "  encoder_with_projection_head.summary()\n",
        "                                                            #ytra_a                                  #yval_a\n",
        "  history = encoder_with_projection_head.fit([xtra_a,xtra_ac], ytra_ac , validation_data =([xval_a,xval_ac],yval_ac), batch_size=32, epochs=100, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
        "\n",
        "\n",
        "  learning_rate = 0.0005\n",
        "  batch_size = 16\n",
        "  hidden_units = 64\n",
        "  projection_units = 128\n",
        "  num_epochs = 200\n",
        "  dropout_rate = 0.3\n",
        "  num_classes = 2\n",
        "  input_shape1 = (19,)\n",
        "  input_shape2 = (38,)\n",
        "\n",
        "  from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
        "  classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
        "  classifier.summary()\n",
        "  history = classifier.fit(x=[xtra_a,xtra_ac], y=ytra_ac, validation_data =([xval_a,xval_ac],yval_ac), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
        "\n",
        "  accuracy = classifier.evaluate([xtest_a,xtest_ac], ytest)[1]\n",
        "\n",
        "  ##  Accuracy on post processed labels (each word should contain only one stressed syllable)\n",
        "  pred_output= classifier.predict([xtest_a,xtest_ac])\n",
        "  # pred_labels= pred_output.argmax(axis =1)\n",
        "  pred1_labels = pred_output[:,1]\n",
        "  post_labels = make_partitions(wtest, pred1_labels)\n",
        "  post_accuracy = calculate_accuracy(post_labels, ytest)\n",
        "\n",
        "  F1_score_WoPP = f1_score(ytest, pred_output.argmax(axis =1))\n",
        "  F1_score_WPP = f1_score(ytest, post_labels)\n",
        "\n",
        "  print(f\"Test accuracy: {round(accuracy * 100, 3)}%\")\n",
        "  print(f\"Postprocessing Test accuracy: {round(post_accuracy * 100, 3)}%\")\n",
        "  print(f\"Test F1_score: {round(F1_score_WoPP * 100, 3)}%\")\n",
        "  print(f\"Postprocessing F1_score: {round(F1_score_WPP * 100, 3)}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
